# -*- coding: utf-8 -*-
"""lgê³µì‹ì ‘ìˆ˜ì„¼í„°_aií”„ë¡œì íŠ¸_ver2.1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dvzSlNRGfWZmxAsA7qtk-RuxSMVGErGC
"""

# ================================================================================
# ë‹´ë‹¹ìë³„ ë¦¬ë“œ ì „í™˜ ì˜ˆì¸¡ íŠ¹í™” ëŒ€ì‹œë³´ë“œ - AI/ML ì„±ëŠ¥ ê°œì„  ë²„ì „
# í”Œë«í¼: Google Colab / Gradio 5.0+
# ëª©í‘œ: ê³ ì„±ëŠ¥ ë‹´ë‹¹ìë³„ ë¦¬ë“œ ì „í™˜ìœ¨ ì˜ˆì¸¡ ë° ìµœì  ë°°ì • ì‹œìŠ¤í…œ
# ================================================================================

# 1. Google Colab í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ìë™ ì„¤ì¹˜
import subprocess
import sys
import os

def install_packages():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜"""
    packages = [
        'gradio>=5.0.0',
        'pandas',
        'numpy',
        'scikit-learn>=1.0.0',  # ë²„ì „ ëª…ì‹œ
        'lightgbm',
        'xgboost',
        'catboost',  # ì¶”ê°€: ë” ê°•ë ¥í•œ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…
        'optuna>=3.0.0',  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ë²„ì „ ëª…ì‹œ)
        'plotly',
        'seaborn',
        'matplotlib',
        'openpyxl',
        'japanize-matplotlib',
        'tensorflow',
        'imbalanced-learn>=0.9.0',  # ì¶”ê°€: í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (ë²„ì „ ëª…ì‹œ)
        'feature-engine'  # ì¶”ê°€: ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    ]

    print("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...")

    # ë¨¼ì € pip ì—…ê·¸ë ˆì´ë“œ
    try:
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip'])
    except:
        pass

    # ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜
    for package in packages:
        try:
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])
            print(f"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ")
        except:
            print(f"âš ï¸ {package} ì„¤ì¹˜ ì‹¤íŒ¨, ê³„ì† ì§„í–‰...")

    # ì‹œê³„ì—´ íŒ¨í‚¤ì§€ëŠ” ë³„ë„ë¡œ ì²˜ë¦¬ (ì˜¤ë¥˜ê°€ ìì£¼ ë°œìƒ)
    print("\nğŸ“ˆ ì‹œê³„ì—´ ë¶„ì„ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...")

    # statsmodels ì„¤ì¹˜ ì‹œë„
    try:
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'statsmodels>=0.13.0'])
        print("âœ… statsmodels ì„¤ì¹˜ ì™„ë£Œ")
    except:
        print("âš ï¸ statsmodels ì„¤ì¹˜ ì‹¤íŒ¨ - ARIMA ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # pmdarima ì„¤ì¹˜ ì‹œë„ (ì˜ì¡´ì„±ì´ ë³µì¡í•´ì„œ ìì£¼ ì‹¤íŒ¨í•¨)
    try:
        # ë¨¼ì € ì˜ì¡´ì„± ì„¤ì¹˜
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'cython', 'numpy', 'scipy'])
        # pmdarima ì„¤ì¹˜
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pmdarima>=2.0.0'])
        print("âœ… pmdarima ì„¤ì¹˜ ì™„ë£Œ")
    except:
        print("âš ï¸ pmdarima ì„¤ì¹˜ ì‹¤íŒ¨ - Auto ARIMAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # prophet ì„¤ì¹˜ ì‹œë„
    try:
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'prophet>=1.1'])
        print("âœ… prophet ì„¤ì¹˜ ì™„ë£Œ")
    except:
        print("âš ï¸ prophet ì„¤ì¹˜ ì‹¤íŒ¨ - Prophet ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # í•œê¸€ í°íŠ¸ ì„¤ì¹˜ (Colabìš©)
    try:
        print("\nğŸ¨ í•œê¸€ í°íŠ¸ ì„¤ì¹˜ ì¤‘...")
        subprocess.run(['apt-get', 'update', '-qq'], check=True, capture_output=True)
        subprocess.run(['apt-get', 'install', '-qq', 'fonts-nanum*'], check=True, capture_output=True)
        subprocess.run(['fc-cache', '-fv'], capture_output=True)
    except:
        print("âš ï¸ í•œê¸€ í°íŠ¸ ì„¤ì¹˜ ì‹¤íŒ¨, ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©")

    print("\nâœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!")
    print("ğŸ’¡ ì¼ë¶€ ì‹œê³„ì—´ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ë„ ê¸°ë³¸ ML ëª¨ë¸ì€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤í–‰
install_packages()

# 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import gradio as gr
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    import japanize_matplotlib
    japanize_matplotlib.japanize()
except:
    # ëŒ€ì²´ í°íŠ¸ ì„¤ì •
    plt.rcParams['font.family'] = ['DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, TimeSeriesSplit
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.metrics import precision_recall_curve, average_precision_score, mean_squared_error
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, VarianceThreshold
import xgboost as xgb
import lightgbm as lgb
from datetime import datetime, timedelta
import io
import json
import base64
from typing import Dict, List, Tuple, Optional, Any

# ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import catboost as cb
    from catboost import CatBoostClassifier
    CATBOOST_AVAILABLE = True
except:
    print("âš ï¸ CatBoostë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    CATBOOST_AVAILABLE = False

try:
    import optuna
    OPTUNA_AVAILABLE = True
except:
    print("âš ï¸ Optunaë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    OPTUNA_AVAILABLE = False

try:
    from imblearn.over_sampling import SMOTE, ADASYN
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.combine import SMOTEENN
    IMBALANCED_AVAILABLE = True
except:
    print("âš ï¸ Imbalanced-learnì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    IMBALANCED_AVAILABLE = False

# ì‹œê³„ì—´ ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    TENSORFLOW_AVAILABLE = True
    print("âœ… TensorFlow ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ TensorFlowë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. LSTM ëª¨ë¸ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤. (ì˜¤ë¥˜: {str(e)})")
    TENSORFLOW_AVAILABLE = False

try:
    from statsmodels.tsa.arima.model import ARIMA
    from statsmodels.tsa.stattools import adfuller
    import pmdarima as pm
    STATSMODELS_AVAILABLE = True
    print("âœ… Statsmodels/pmdarima ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ Statsmodelsë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ARIMA ëª¨ë¸ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    print(f"   ì›ì¸: {str(e)}")
    print("   í•´ê²°ë°©ë²•: !pip install statsmodels pmdarima")
    STATSMODELS_AVAILABLE = False
except Exception as e:
    print(f"âš ï¸ Statsmodels ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
    STATSMODELS_AVAILABLE = False

try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
    print("âœ… Prophet ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ Prophetì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì˜¤ë¥˜: {str(e)})")
    PROPHET_AVAILABLE = False

# ================================================================================
# 3. ì „ì—­ ë³€ìˆ˜ ë° ìƒíƒœ ê´€ë¦¬
# ================================================================================

class AppState:
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœ ê´€ë¦¬ í´ë˜ìŠ¤"""
    def __init__(self):
        self.data = None
        self.processed_data = None
        self.best_model = None
        self.best_model_name = None
        self.feature_names = None
        self.encoders = None
        self.model_results = {}
        self.manager_performance = None
        self.prediction_results = None
        self.feature_importance = None
        self.is_data_loaded = False
        self.is_preprocessed = False
        self.is_model_trained = False
        self.time_series_models = {}  # ì‹œê³„ì—´ ëª¨ë¸ ì €ì¥
        self.ensemble_model = None  # ì•™ìƒë¸” ëª¨ë¸
        self.feature_selector = None  # í”¼ì²˜ ì„ íƒê¸°
        self.scaler = None  # ìŠ¤ì¼€ì¼ëŸ¬

app_state = AppState()

# ================================================================================
# 4. ìƒ˜í”Œ ë°ì´í„° ìƒì„± í•¨ìˆ˜
# ================================================================================

def create_sample_data() -> pd.DataFrame:
    """tst_atalk.xlsxì™€ ë™ì¼í•œ êµ¬ì¡°ì˜ ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    n_samples = 500

    # ë‹´ë‹¹ì ëª©ë¡ (ì‹¤ì œ ë°ì´í„°ì™€ ë™ì¼)
    managers = [
        'ë°°ì œê²½(sv5203)', 'ì˜ˆì‹ í•´(sv0027)', 'ì–‘ì€ì„œ(sv5200)',
        'ê¹€íƒœì—°(sv0026)', 'ìœ¤ì„í•œ(sv5202)', 'ì •ì§€ì›…(sv0025)',
        'ë°•ì˜¨ì„¤ì•„(sv0024)', None  # Noneì€ ë¯¸ë°°ì •
    ]

    # íŒë§¤ì²˜ ëª©ë¡
    channels = ['Gë§ˆì¼“', 'CJëª°', '11ë²ˆê°€', 'SSG', 'ì˜¥ì…˜', 'ë„¤ì´ë²„', 'ì¿ íŒ¡']

    # ìƒíƒœ ëª©ë¡
    statuses = ['ìŠ¹ì¸ì™„ë£Œ', '1íšŒ ì ‘ì´‰', '2íšŒ ì ‘ì´‰', '3íšŒ ì ‘ì´‰', 'ì ‘ì´‰ì‹¤íŒ¨', 'ì·¨ì†Œì™„ë£Œ', 'ì ‘ìˆ˜ì™„ë£Œ']
    status_weights = [0.25, 0.15, 0.20, 0.10, 0.10, 0.05, 0.15]

    # ë°ì´í„° ìƒì„±
    data = {
        'ê³µìœ ': ['ê³µìš©'] * n_samples,
        'ì´ë¦„': [f'ê³ ê°_{np.random.randint(1000, 9999)}' for _ in range(n_samples)],
        'ê³ ê°ë²ˆí˜¸': [f'452036_{i}' for i in range(n_samples)],
        'ì„±ë³„': np.random.choice(['ë‚¨ì', 'ì—¬ì'], n_samples, p=[0.4, 0.6]),
        'ìœ í˜•': np.random.choice(['ê°œì¸', 'ë²•ì¸'], n_samples, p=[0.9, 0.1]),
        'ìƒì¼êµ¬ë¶„': ['ì–‘ë ¥'] * n_samples,
        'í˜¼ì¸ì—¬ë¶€': np.random.choice(['ë¯¸í˜¼', 'ê¸°í˜¼'], n_samples, p=[0.4, 0.6]),
        'TEL': [f'10{np.random.randint(10000000, 99999999)}' for _ in range(n_samples)],
        'ë©”ëª¨': ['LGì •ìˆ˜ê¸°ë Œíƒˆ ìƒë‹´ ë¬¸ì˜ ' * np.random.randint(1, 5) for _ in range(n_samples)],
        'ìƒíƒœ': np.random.choice(statuses, n_samples, p=status_weights),
        'ë§ˆì¼€íŒ…í™œìš©ë™ì˜': np.random.choice(['ë™ì˜', 'ë¯¸ë™ì˜'], n_samples, p=[0.7, 0.3]),
        'ì´ë©”ì¼ìˆ˜ì‹ ë™ì˜': np.random.choice(['ë™ì˜', 'ë¯¸ë™ì˜'], n_samples, p=[0.6, 0.4]),
        'ë¬¸ììˆ˜ì‹ ë™ì˜': np.random.choice(['ë™ì˜', 'ë¯¸ë™ì˜'], n_samples, p=[0.65, 0.35]),
        'ë‹´ë‹¹ì': np.random.choice(managers, n_samples, p=[0.2, 0.17, 0.16, 0.15, 0.15, 0.13, 0.03, 0.01]),
        'ë“±ë¡ì¼ì': pd.date_range(start='2024-01-01', periods=n_samples, freq='H'),
        'íŒë§¤ì²˜': np.random.choice(channels, n_samples),
        'ë“±ë¡ì': np.random.choice(['ì–‘ì€ì„œ', 'ê¹€ë¯¼ìˆ˜', 'ì´ì§€ì€'], n_samples),
        'ë“±ë¡ì¼ì‹œ': pd.date_range(start='2024-01-01', periods=n_samples, freq='H')
    }

    df = pd.DataFrame(data)

    # ë‹´ë‹¹ìë³„ë¡œ ë‹¤ë¥¸ ìŠ¹ì¸ìœ¨ ì ìš©
    for idx, row in df.iterrows():
        if pd.notna(row['ë‹´ë‹¹ì']):
            if 'ë°°ì œê²½' in row['ë‹´ë‹¹ì']:
                if np.random.random() < 0.35:
                    df.at[idx, 'ìƒíƒœ'] = 'ìŠ¹ì¸ì™„ë£Œ'
            elif 'ì˜ˆì‹ í•´' in row['ë‹´ë‹¹ì']:
                if np.random.random() < 0.32:
                    df.at[idx, 'ìƒíƒœ'] = 'ìŠ¹ì¸ì™„ë£Œ'
            elif 'ê¹€íƒœì—°' in row['ë‹´ë‹¹ì']:
                if np.random.random() < 0.30:
                    df.at[idx, 'ìƒíƒœ'] = 'ìŠ¹ì¸ì™„ë£Œ'

    return df

# ================================================================================
# 5. ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜
# ================================================================================

def advanced_feature_engineering(data: pd.DataFrame) -> pd.DataFrame:
    """ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""

    # ë‹´ë‹¹ìë³„ ê³¼ê±° ì„±ê³¼ í†µê³„
    if 'ë‹´ë‹¹ìëª…' in data.columns:
        manager_stats = data.groupby('ë‹´ë‹¹ìëª…').agg({
            'is_approved': ['mean', 'sum', 'count'],
            'ì ‘ì´‰íšŸìˆ˜': ['mean', 'std'],
            'ë©”ëª¨_ê¸¸ì´': 'mean'
        }).round(4)

        manager_stats.columns = [
            'ë‹´ë‹¹ì_ìŠ¹ì¸ìœ¨', 'ë‹´ë‹¹ì_ì´ìŠ¹ì¸ìˆ˜', 'ë‹´ë‹¹ì_ì´ë¦¬ë“œìˆ˜',
            'ë‹´ë‹¹ì_í‰ê· ì ‘ì´‰', 'ë‹´ë‹¹ì_ì ‘ì´‰í¸ì°¨', 'ë‹´ë‹¹ì_í‰ê· ë©”ëª¨'
        ]

        # ë‹´ë‹¹ì í†µê³„ ë³‘í•©
        data = data.merge(manager_stats, left_on='ë‹´ë‹¹ìëª…', right_index=True, how='left')

        # ë‹´ë‹¹ì ê²½í—˜ ì§€í‘œ
        data['ë‹´ë‹¹ì_ê²½í—˜ì§€ìˆ˜'] = data['ë‹´ë‹¹ì_ì´ë¦¬ë“œìˆ˜'] / data['ë‹´ë‹¹ì_ì´ë¦¬ë“œìˆ˜'].max()
        data['ë‹´ë‹¹ì_íš¨ìœ¨ì„±'] = data['ë‹´ë‹¹ì_ìŠ¹ì¸ìœ¨'] / (data['ë‹´ë‹¹ì_í‰ê· ì ‘ì´‰'] + 1)

    # ì‹œê°„ëŒ€ë³„ íŒ¨í„´
    if 'ë“±ë¡ì¼ì' in data.columns and pd.api.types.is_datetime64_any_dtype(data['ë“±ë¡ì¼ì']):
        # ì‹œê°„ëŒ€ë³„ íŠ¹ì„±
        data['ì‹œê°„ëŒ€'] = pd.cut(data['ë“±ë¡_ì‹œê°„'],
                               bins=[0, 6, 12, 18, 24],
                               labels=['ìƒˆë²½', 'ì˜¤ì „', 'ì˜¤í›„', 'ì €ë…'])

        # ì£¼ë§ ì—¬ë¶€
        data['ì£¼ë§ì—¬ë¶€'] = (data['ë“±ë¡_ìš”ì¼'] >= 5).astype(int)

        # ì›”ì´ˆ/ì›”ë§ êµ¬ë¶„
        data['ì›”ì´ˆ'] = (data['ë“±ë¡_ì¼'] <= 10).astype(int)
        data['ì›”ë§'] = (data['ë“±ë¡_ì¼'] >= 21).astype(int)

        # ë¶„ê¸°
        data['ë¶„ê¸°'] = data['ë“±ë¡ì¼ì'].dt.quarter

    # íŒë§¤ì²˜ë³„ íŠ¹ì„±
    if 'íŒë§¤ì²˜' in data.columns:
        channel_stats = data.groupby('íŒë§¤ì²˜')['is_approved'].agg(['mean', 'count'])
        channel_stats.columns = ['íŒë§¤ì²˜_í‰ê· ìŠ¹ì¸ìœ¨', 'íŒë§¤ì²˜_ë¹ˆë„']
        data = data.merge(channel_stats, left_on='íŒë§¤ì²˜', right_index=True, how='left')

    # ê³ ê° íŠ¹ì„± ì¡°í•©
    if all(col in data.columns for col in ['ì„±ë³„', 'í˜¼ì¸ì—¬ë¶€', 'ìœ í˜•']):
        data['ê³ ê°í”„ë¡œíŒŒì¼'] = data['ì„±ë³„'] + '_' + data['í˜¼ì¸ì—¬ë¶€'] + '_' + data['ìœ í˜•']

        # í”„ë¡œíŒŒì¼ë³„ ìŠ¹ì¸ìœ¨
        profile_stats = data.groupby('ê³ ê°í”„ë¡œíŒŒì¼')['is_approved'].mean()
        data['í”„ë¡œíŒŒì¼_ìŠ¹ì¸ìœ¨'] = data['ê³ ê°í”„ë¡œíŒŒì¼'].map(profile_stats)

    # ë™ì˜ ì ìˆ˜
    consent_cols = ['ë§ˆì¼€íŒ…í™œìš©ë™ì˜_num', 'ì´ë©”ì¼ìˆ˜ì‹ ë™ì˜_num', 'ë¬¸ììˆ˜ì‹ ë™ì˜_num']
    if all(col in data.columns for col in consent_cols):
        data['ë™ì˜ì ìˆ˜'] = data[consent_cols].sum(axis=1)
        data['ì™„ì „ë™ì˜'] = (data['ë™ì˜ì ìˆ˜'] == 3).astype(int)

    # ë©”ëª¨ ê´€ë ¨ ê³ ê¸‰ í”¼ì²˜
    if 'ë©”ëª¨' in data.columns:
        # íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
        keywords = ['ì •ìˆ˜ê¸°', 'ë Œíƒˆ', 'ìƒë‹´', 'ë¬¸ì˜', 'ê²¬ì ', 'ì„¤ì¹˜']
        for keyword in keywords:
            data[f'ë©”ëª¨_{keyword}_í¬í•¨'] = data['ë©”ëª¨'].str.contains(keyword, na=False).astype(int)

        # ë©”ëª¨ í’ˆì§ˆ ì ìˆ˜
        data['ë©”ëª¨_í’ˆì§ˆì ìˆ˜'] = (
            data['ë©”ëª¨_ê¸¸ì´'] / data['ë©”ëª¨_ê¸¸ì´'].max() * 0.3 +
            data['ë©”ëª¨_ë‹¨ì–´ìˆ˜'] / data['ë©”ëª¨_ë‹¨ì–´ìˆ˜'].max() * 0.3 +
            data[[f'ë©”ëª¨_{kw}_í¬í•¨' for kw in keywords]].sum(axis=1) / len(keywords) * 0.4
        )

    # ìƒí˜¸ì‘ìš© í”¼ì²˜
    if 'ë‹´ë‹¹ì_ìŠ¹ì¸ìœ¨' in data.columns and 'íŒë§¤ì²˜_í‰ê· ìŠ¹ì¸ìœ¨' in data.columns:
        data['ë‹´ë‹¹ìXíŒë§¤ì²˜_ì‹œë„ˆì§€'] = data['ë‹´ë‹¹ì_ìŠ¹ì¸ìœ¨'] * data['íŒë§¤ì²˜_í‰ê· ìŠ¹ì¸ìœ¨']

    if 'ì ‘ì´‰íšŸìˆ˜' in data.columns and 'ë©”ëª¨_ê¸¸ì´' in data.columns:
        data['ì ‘ì´‰Xë©”ëª¨_ìƒí˜¸ì‘ìš©'] = data['ì ‘ì´‰íšŸìˆ˜'] * np.log1p(data['ë©”ëª¨_ê¸¸ì´'])

    # ì´ìƒì¹˜ í”Œë˜ê·¸
    numerical_cols = data.select_dtypes(include=[np.number]).columns
    for col in numerical_cols:
        if col not in ['is_approved', 'is_approved_binary']:
            Q1 = data[col].quantile(0.25)
            Q3 = data[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - 1.5 * IQR
            upper = Q3 + 1.5 * IQR
            data[f'{col}_ì´ìƒì¹˜'] = ((data[col] < lower) | (data[col] > upper)).astype(int)

    return data

# ================================================================================
# 6. ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)
# ================================================================================

def load_data(file) -> Tuple[pd.DataFrame, str]:
    """ë°ì´í„° ë¡œë“œ ë° ì´ˆê¸° ë¶„ì„"""
    try:
        if file is None:
            # ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©
            df = create_sample_data()
            info_text = "ğŸ“Œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
        else:
            # íŒŒì¼ ì½ê¸°
            filename = file.name if hasattr(file, 'name') else 'uploaded_file'

            try:
                if filename.endswith('.csv'):
                    df = pd.read_csv(file, encoding='utf-8')
                elif filename.endswith(('.xlsx', '.xls')):
                    df = pd.read_excel(file)
                else:
                    return None, "âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. CSV ë˜ëŠ” Excel íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
            except Exception as e:
                return None, f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"

        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['ìƒíƒœ', 'ë‹´ë‹¹ì']
        missing_columns = [col for col in required_columns if col not in df.columns]

        if missing_columns:
            return None, f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_columns)}"

        # ë°ì´í„° ê¸°ë³¸ ì •ë³´ ë¶„ì„
        total_rows = len(df)

        # ìƒíƒœë³„ ë¶„í¬ ë¶„ì„
        status_dist = df['ìƒíƒœ'].value_counts().to_dict()
        approval_count = df[df['ìƒíƒœ'] == 'ìŠ¹ì¸ì™„ë£Œ'].shape[0]
        approval_rate = (approval_count / total_rows * 100) if total_rows > 0 else 0

        # ë‹´ë‹¹ìë³„ ë¶„í¬ ë¶„ì„
        manager_dist = df['ë‹´ë‹¹ì'].value_counts().head(10).to_dict()

        # íŒë§¤ì²˜ë³„ ë¶„í¬ ë¶„ì„
        channel_dist = {}
        if 'íŒë§¤ì²˜' in df.columns:
            channel_dist = df['íŒë§¤ì²˜'].value_counts().head(10).to_dict()

        info_text = f"""âœ… ë°ì´í„° ë¡œë”© ì„±ê³µ!

ğŸ“Š ë°ì´í„° ê°œìš”:
- ì „ì²´ ë¦¬ë“œ ìˆ˜: {total_rows:,}ê°œ
- ìŠ¹ì¸ì™„ë£Œ: {approval_count}ê±´ ({approval_rate:.1f}%)
- ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ

ğŸ“ˆ ìƒíƒœë³„ ë¶„í¬:
{chr(10).join([f'- {status}: {count}ê±´' for status, count in list(status_dist.items())[:5]])}

ğŸ‘¥ ìƒìœ„ ë‹´ë‹¹ì (ë¦¬ë“œ ìˆ˜):
{chr(10).join([f'- {manager}: {count}ê±´' for manager, count in list(manager_dist.items())[:5]])}

ğŸª ì£¼ìš” íŒë§¤ì²˜:
{chr(10).join([f'- {channel}: {count}ê±´' for channel, count in list(channel_dist.items())[:3]])}

âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ! ì´ì œ [ì „ì²˜ë¦¬ ì‹¤í–‰] ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.
"""

        app_state.data = df
        app_state.is_data_loaded = True
        return df, info_text

    except Exception as e:
        return None, f"âŒ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def preprocess_for_prediction(df: pd.DataFrame = None) -> Tuple[pd.DataFrame, str]:
    """ë¦¬ë“œ ì „í™˜ ì˜ˆì¸¡ì„ ìœ„í•œ ê³ ê¸‰ ë°ì´í„° ì „ì²˜ë¦¬"""
    try:
        if df is None:
            if app_state.data is None:
                return None, "âŒ ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”."
            df = app_state.data

        data = df.copy()

        # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
        data['is_approved'] = (data['ìƒíƒœ'] == 'ìŠ¹ì¸ì™„ë£Œ').astype(int)

        # ë‹´ë‹¹ì ì •ë³´ ì •ì œ
        data['ë‹´ë‹¹ì'] = data['ë‹´ë‹¹ì'].fillna('ë¯¸ë°°ì •')
        data['ë‹´ë‹¹ìëª…'] = data['ë‹´ë‹¹ì'].str.extract(r'([^(]+)')[0].str.strip()
        data['ë‹´ë‹¹ìëª…'] = data['ë‹´ë‹¹ìëª…'].fillna('ë¯¸ë°°ì •')

        # ë‚ ì§œ ì²˜ë¦¬
        if 'ë“±ë¡ì¼ì' in data.columns:
            data['ë“±ë¡ì¼ì'] = pd.to_datetime(data['ë“±ë¡ì¼ì'], errors='coerce')
            data['ë“±ë¡_ë…„'] = data['ë“±ë¡ì¼ì'].dt.year
            data['ë“±ë¡_ì›”'] = data['ë“±ë¡ì¼ì'].dt.month
            data['ë“±ë¡_ì¼'] = data['ë“±ë¡ì¼ì'].dt.day
            data['ë“±ë¡_ìš”ì¼'] = data['ë“±ë¡ì¼ì'].dt.dayofweek
            data['ë“±ë¡_ì‹œê°„'] = data['ë“±ë¡ì¼ì'].dt.hour
        else:
            # ê¸°ë³¸ê°’ ì„¤ì •
            data['ë“±ë¡_ë…„'] = 2024
            data['ë“±ë¡_ì›”'] = 1
            data['ë“±ë¡_ì¼'] = 1
            data['ë“±ë¡_ìš”ì¼'] = 0
            data['ë“±ë¡_ì‹œê°„'] = 9

        # ì ‘ì´‰ íšŸìˆ˜ ì¶”ì¶œ
        data['ì ‘ì´‰íšŸìˆ˜'] = 0
        contact_pattern = r'(\d+)íšŒ ì ‘ì´‰'
        contact_matches = data['ìƒíƒœ'].str.extract(contact_pattern)
        data.loc[contact_matches[0].notna(), 'ì ‘ì´‰íšŸìˆ˜'] = contact_matches[0].fillna(0).astype(int)

        # ë©”ëª¨ ê¸¸ì´ ê³„ì‚°
        if 'ë©”ëª¨' in data.columns:
            data['ë©”ëª¨_ê¸¸ì´'] = data['ë©”ëª¨'].fillna('').str.len()
            data['ë©”ëª¨_ë‹¨ì–´ìˆ˜'] = data['ë©”ëª¨'].fillna('').str.split().str.len()
        else:
            data['ë©”ëª¨_ê¸¸ì´'] = 50
            data['ë©”ëª¨_ë‹¨ì–´ìˆ˜'] = 10

        # ë™ì˜ ì •ë³´ ìˆ˜ì¹˜í™”
        consent_cols = ['ë§ˆì¼€íŒ…í™œìš©ë™ì˜', 'ì´ë©”ì¼ìˆ˜ì‹ ë™ì˜', 'ë¬¸ììˆ˜ì‹ ë™ì˜']
        for col in consent_cols:
            if col in data.columns:
                data[f'{col}_num'] = (data[col] == 'ë™ì˜').astype(int)
            else:
                data[f'{col}_num'] = 1  # ê¸°ë³¸ê°’

        # ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì ìš©
        data = advanced_feature_engineering(data)

        # ë‹´ë‹¹ìë³„ ì„±ê³¼ í†µê³„ ê³„ì‚°
        manager_stats = data[data['ë‹´ë‹¹ìëª…'] != 'ë¯¸ë°°ì •'].groupby('ë‹´ë‹¹ìëª…').agg({
            'is_approved': ['count', 'sum', 'mean'],
            'ì ‘ì´‰íšŸìˆ˜': 'mean'
        }).round(3)

        if len(manager_stats) > 0:
            manager_stats.columns = ['ì´_ë¦¬ë“œìˆ˜', 'ìŠ¹ì¸ì™„ë£Œìˆ˜', 'ìŠ¹ì¸ì™„ë£Œìœ¨', 'í‰ê· _ì ‘ì´‰íšŸìˆ˜']
            manager_stats = manager_stats.reset_index()
            manager_stats = manager_stats.sort_values('ìŠ¹ì¸ì™„ë£Œìœ¨', ascending=False)
        else:
            manager_stats = pd.DataFrame()

        # ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
        feature_count = len([col for col in data.columns if col not in df.columns])

        result_text = f"""âœ… ê³ ê¸‰ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!

ğŸ“Š íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬:
- ìŠ¹ì¸ì™„ë£Œ: {data['is_approved'].sum():,}ê±´ ({data['is_approved'].mean():.1%})
- ë¯¸ìŠ¹ì¸: {(1-data['is_approved']).sum():,}ê±´

ğŸ”§ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§:
- ìƒì„±ëœ ì‹ ê·œ í”¼ì²˜: {feature_count}ê°œ
- ë‹´ë‹¹ìë³„ ì„±ê³¼ ì§€í‘œ ì¶”ê°€
- ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ì¶”ì¶œ
- ê³ ê° í”„ë¡œíŒŒì¼ ë¶„ì„
- ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„±

ğŸ‘¥ ë‹´ë‹¹ìë³„ ì„±ê³¼ (ìƒìœ„ 5ëª…):
{manager_stats.head().to_string(index=False) if len(manager_stats) > 0 else 'ë‹´ë‹¹ì ë°ì´í„° ì—†ìŒ'}

ğŸ“ˆ ì£¼ìš” í”¼ì²˜ ìƒì„±:
- ë‹´ë‹¹ì ê²½í—˜ ë° íš¨ìœ¨ì„± ì§€í‘œ
- ì‹œê°„ëŒ€ë³„ íŒ¨í„´ (ì£¼ë§, ì‹œê°„ëŒ€, ì›”ì´ˆ/ì›”ë§)
- íŒë§¤ì²˜ë³„ ì„±ê³¼ ì§€í‘œ
- ê³ ê° í”„ë¡œíŒŒì¼ë³„ ìŠ¹ì¸ìœ¨
- ë©”ëª¨ í’ˆì§ˆ ì ìˆ˜
- ì´ìƒì¹˜ íƒì§€ í”Œë˜ê·¸

âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ì œ ë‹¤ë¥¸ íƒ­ì—ì„œ ë¶„ì„ì„ ì§„í–‰í•˜ì„¸ìš”.
"""

        app_state.processed_data = data
        app_state.manager_performance = manager_stats
        app_state.is_preprocessed = True
        return data, result_text

    except Exception as e:
        return None, f"âŒ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\nìƒì„¸ ì˜¤ë¥˜:\n{type(e).__name__}: {str(e)}"

# ================================================================================
# 7. ë‹´ë‹¹ìë³„ ì„±ê³¼ ë¶„ì„ ë° ì‹œê°í™” (ê¸°ì¡´ ìœ ì§€)
# ================================================================================

def create_manager_performance_dashboard() -> Tuple[List[go.Figure], str]:
    """ë‹´ë‹¹ìë³„ ì„±ê³¼ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
    try:
        if not app_state.is_preprocessed or app_state.processed_data is None:
            return [], "âŒ ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."

        data = app_state.processed_data
        plots = []

        # 1. ë‹´ë‹¹ìë³„ ìŠ¹ì¸ì™„ë£Œìœ¨ ë° ì²˜ë¦¬ëŸ‰
        manager_stats = data[data['ë‹´ë‹¹ìëª…'] != 'ë¯¸ë°°ì •'].groupby('ë‹´ë‹¹ìëª…').agg({
            'is_approved': ['count', 'sum', 'mean']
        }).round(3)

        if len(manager_stats) == 0:
            return [], "âŒ ë‹´ë‹¹ì ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        manager_stats.columns = ['ì´_ë¦¬ë“œìˆ˜', 'ìŠ¹ì¸ì™„ë£Œìˆ˜', 'ìŠ¹ì¸ì™„ë£Œìœ¨']
        manager_stats = manager_stats.reset_index()
        manager_stats = manager_stats[manager_stats['ì´_ë¦¬ë“œìˆ˜'] >= 5]  # 5ê±´ ì´ìƒë§Œ í‘œì‹œ
        manager_stats = manager_stats.sort_values('ìŠ¹ì¸ì™„ë£Œìœ¨', ascending=False)

        # ê·¸ë˜í”„ 1: ë‹´ë‹¹ìë³„ ì„±ê³¼ ë§¤íŠ¸ë¦­ìŠ¤
        fig1 = px.scatter(manager_stats,
                         x='ì´_ë¦¬ë“œìˆ˜',
                         y='ìŠ¹ì¸ì™„ë£Œìœ¨',
                         size='ìŠ¹ì¸ì™„ë£Œìˆ˜',
                         text='ë‹´ë‹¹ìëª…',
                         color='ìŠ¹ì¸ì™„ë£Œìœ¨',
                         title='ë‹´ë‹¹ìë³„ ì„±ê³¼ ë§¤íŠ¸ë¦­ìŠ¤ (ë¦¬ë“œ ì²˜ë¦¬ëŸ‰ vs ìŠ¹ì¸ì™„ë£Œìœ¨)',
                         color_continuous_scale='RdYlGn')

        fig1.update_traces(textposition='top center', textfont_size=10)
        fig1.update_layout(
            height=500,
            xaxis_title='ì´ ë¦¬ë“œ ì²˜ë¦¬ìˆ˜',
            yaxis_title='ìŠ¹ì¸ì™„ë£Œìœ¨',
            yaxis_tickformat='.0%'
        )

        # í‰ê· ì„  ì¶”ê°€
        avg_approval_rate = data['is_approved'].mean()
        fig1.add_hline(y=avg_approval_rate, line_dash="dash", line_color="gray",
                       annotation_text=f"ì „ì²´ í‰ê· : {avg_approval_rate:.1%}")

        plots.append(fig1)

        # ê·¸ë˜í”„ 2: ë‹´ë‹¹ìë³„ ìŠ¹ì¸ì™„ë£Œìœ¨ ìˆœìœ„
        fig2 = go.Figure()

        # ìƒìœ„ 10ëª…
        top_managers = manager_stats.head(10)
        fig2.add_trace(go.Bar(
            x=top_managers['ë‹´ë‹¹ìëª…'],
            y=top_managers['ìŠ¹ì¸ì™„ë£Œìœ¨'],
            text=top_managers['ìŠ¹ì¸ì™„ë£Œìœ¨'].apply(lambda x: f'{x:.1%}'),
            textposition='outside',
            name='ìŠ¹ì¸ì™„ë£Œìœ¨',
            marker_color='lightgreen'
        ))

        fig2.update_layout(
            title='ë‹´ë‹¹ìë³„ ìŠ¹ì¸ì™„ë£Œìœ¨ TOP 10',
            xaxis_title='ë‹´ë‹¹ì',
            yaxis_title='ìŠ¹ì¸ì™„ë£Œìœ¨',
            yaxis_tickformat='.0%',
            height=400
        )

        plots.append(fig2)

        # ê·¸ë˜í”„ 3: ë‹´ë‹¹ìë³„ ì¼ì¼ ì²˜ë¦¬ íŒ¨í„´
        if 'ë“±ë¡ì¼ì' in data.columns and pd.api.types.is_datetime64_any_dtype(data['ë“±ë¡ì¼ì']):
            daily_stats = data.groupby(['ë‹´ë‹¹ìëª…', pd.Grouper(key='ë“±ë¡ì¼ì', freq='D')]).agg({
                'is_approved': ['count', 'mean']
            }).reset_index()
            daily_stats.columns = ['ë‹´ë‹¹ìëª…', 'ë‚ ì§œ', 'ì²˜ë¦¬ê±´ìˆ˜', 'ìŠ¹ì¸ì™„ë£Œìœ¨']

            # ìƒìœ„ 5ëª… ë‹´ë‹¹ìë§Œ í‘œì‹œ
            top_5_managers = manager_stats.head(5)['ë‹´ë‹¹ìëª…'].tolist()
            daily_top = daily_stats[daily_stats['ë‹´ë‹¹ìëª…'].isin(top_5_managers)]

            if len(daily_top) > 0:
                fig3 = px.line(daily_top,
                              x='ë‚ ì§œ',
                              y='ìŠ¹ì¸ì™„ë£Œìœ¨',
                              color='ë‹´ë‹¹ìëª…',
                              title='ìƒìœ„ 5ëª… ë‹´ë‹¹ìì˜ ì¼ë³„ ìŠ¹ì¸ì™„ë£Œìœ¨ ì¶”ì´',
                              markers=True)

                fig3.update_layout(
                    height=400,
                    yaxis_tickformat='.0%',
                    hovermode='x unified'
                )

                plots.append(fig3)

        # ê·¸ë˜í”„ 4: ë‹´ë‹¹ìë³„ ì ‘ì´‰ íš¨ìœ¨ì„±
        contact_stats = data[data['ë‹´ë‹¹ìëª…'] != 'ë¯¸ë°°ì •'].groupby('ë‹´ë‹¹ìëª…').agg({
            'ì ‘ì´‰íšŸìˆ˜': 'mean',
            'is_approved': 'mean'
        }).reset_index()

        contact_stats = contact_stats[contact_stats['ì ‘ì´‰íšŸìˆ˜'] > 0]

        if len(contact_stats) > 0:
            contact_stats['íš¨ìœ¨ì„±'] = contact_stats['is_approved'] / (contact_stats['ì ‘ì´‰íšŸìˆ˜'] + 1)  # +1 to avoid division by zero
            contact_stats = contact_stats.sort_values('íš¨ìœ¨ì„±', ascending=False).head(15)

            fig4 = go.Figure()
            fig4.add_trace(go.Bar(
                x=contact_stats['ë‹´ë‹¹ìëª…'],
                y=contact_stats['íš¨ìœ¨ì„±'],
                text=contact_stats['íš¨ìœ¨ì„±'].apply(lambda x: f'{x:.3f}'),
                textposition='outside',
                name='ì ‘ì´‰ íš¨ìœ¨ì„±',
                marker_color='lightcoral'
            ))

            fig4.update_layout(
                title='ë‹´ë‹¹ìë³„ ì ‘ì´‰ íš¨ìœ¨ì„± (ìŠ¹ì¸ìœ¨/í‰ê· ì ‘ì´‰íšŸìˆ˜)',
                xaxis_title='ë‹´ë‹¹ì',
                yaxis_title='íš¨ìœ¨ì„± ì§€ìˆ˜',
                height=400
            )

            plots.append(fig4)

        success_message = "âœ… ì„±ê³¼ ë¶„ì„ ì™„ë£Œ!"
        return plots, success_message

    except Exception as e:
        return [], f"âŒ ì„±ê³¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# ================================================================================
# 8. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
# ================================================================================

def optimize_hyperparameters(X_train, y_train, model_type='xgboost'):
    """Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”)"""
    if not OPTUNA_AVAILABLE:
        return None

    try:
        def objective(trial):
            try:
                if model_type == 'xgboost':
                    params = {
                        'n_estimators': trial.suggest_int('n_estimators', 50, 200),
                        'max_depth': trial.suggest_int('max_depth', 3, 8),
                        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
                        'subsample': trial.suggest_float('subsample', 0.7, 1.0),
                        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),
                        'gamma': trial.suggest_float('gamma', 0, 3),
                        'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),
                        'random_state': 42,
                        'use_label_encoder': False,
                        'eval_metric': 'logloss'
                    }
                    model = xgb.XGBClassifier(**params)

                elif model_type == 'lightgbm':
                    params = {
                        'n_estimators': trial.suggest_int('n_estimators', 50, 200),
                        'max_depth': trial.suggest_int('max_depth', 3, 8),
                        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
                        'num_leaves': trial.suggest_int('num_leaves', 20, 150),
                        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),
                        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),
                        'bagging_freq': trial.suggest_int('bagging_freq', 1, 5),
                        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),
                        'random_state': 42,
                        'verbose': -1
                    }
                    model = lgb.LGBMClassifier(**params)

                elif model_type == 'catboost':
                    if not CATBOOST_AVAILABLE:
                        return 0
                    params = {
                        'iterations': trial.suggest_int('iterations', 50, 150),
                        'depth': trial.suggest_int('depth', 3, 8),
                        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
                        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 5),
                        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 0.5),
                        'random_strength': trial.suggest_float('random_strength', 0, 0.5),
                        'random_state': 42,
                        'verbose': False
                    }
                    model = CatBoostClassifier(**params)

                # ê°„ë‹¨í•œ êµì°¨ ê²€ì¦ (ì‹œê³„ì—´ ë¶„í•  ëŒ€ì‹  ì¼ë°˜ ë¶„í•  ì‚¬ìš©)
                if len(X_train) < 100:
                    # ë°ì´í„°ê°€ ì ì€ ê²½ìš° ë‹¨ìˆœ í•™ìŠµ/ê²€ì¦ ë¶„í• 
                    X_tr, X_val, y_tr, y_val = train_test_split(
                        X_train, y_train, test_size=0.2, random_state=42
                    )
                    model.fit(X_tr, y_tr)
                    y_pred_proba = model.predict_proba(X_val)[:, 1]
                    score = roc_auc_score(y_val, y_pred_proba)
                else:
                    # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° êµì°¨ ê²€ì¦
                    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
                    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')
                    score = scores.mean()

                return score

            except Exception as e:
                print(f"Trial ì‹¤íŒ¨: {str(e)}")
                return 0.0

        # ìµœì í™” ìˆ˜í–‰ (ì‹œë„ íšŸìˆ˜ ì¤„ì„)
        study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))
        study.optimize(objective, n_trials=10, show_progress_bar=False, catch=(Exception,))

        # ì™„ë£Œëœ trialì´ ìˆëŠ”ì§€ í™•ì¸
        if len(study.trials) == 0 or study.best_trial is None:
            print(f"{model_type} ìµœì í™” ì‹¤íŒ¨ - ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©")
            return None

        return study.best_params

    except Exception as e:
        print(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# ================================================================================
# 9. ê°œì„ ëœ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ
# ================================================================================

def handle_class_imbalance(X_train, y_train):
    """í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (ì•ˆì „í•œ ë²„ì „)"""
    if not IMBALANCED_AVAILABLE:
        return X_train, y_train

    try:
        # í´ë˜ìŠ¤ ë¹„ìœ¨ í™•ì¸
        unique, counts = np.unique(y_train, return_counts=True)
        min_class_count = min(counts)

        # ì†Œìˆ˜ í´ë˜ìŠ¤ê°€ ë„ˆë¬´ ì ìœ¼ë©´ SMOTE ì‚¬ìš© ë¶ˆê°€
        if min_class_count < 6:
            print("ì†Œìˆ˜ í´ë˜ìŠ¤ ìƒ˜í”Œì´ ë„ˆë¬´ ì ì–´ ë¶ˆê· í˜• ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return X_train, y_train

        # SMOTEë§Œ ì‚¬ìš© (SMOTEENNì€ ë•Œë•Œë¡œ ë¶ˆì•ˆì •í•¨)
        smote = SMOTE(random_state=42, k_neighbors=min(5, min_class_count-1))
        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

        print(f"í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ ì™„ë£Œ: {len(X_train)} â†’ {len(X_resampled)}")
        return X_resampled, y_resampled

    except Exception as e:
        print(f"í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return X_train, y_train

def select_best_features(X_train, y_train, X_test, feature_names, k=20):
    """ìµœì  í”¼ì²˜ ì„ íƒ (ì•ˆì „í•œ ë²„ì „)"""
    try:
        # í”¼ì²˜ ìˆ˜ê°€ kë³´ë‹¤ ì ìœ¼ë©´ ëª¨ë“  í”¼ì²˜ ì‚¬ìš©
        if X_train.shape[1] <= k:
            return X_train, X_test, feature_names, None

        # ë¶„ì‚°ì´ 0ì¸ í”¼ì²˜ ì œê±°
        var_selector = VarianceThreshold(threshold=0.01)
        X_train_var = var_selector.fit_transform(X_train)
        X_test_var = var_selector.transform(X_test)
        selected_features_var = [feature_names[i] for i in var_selector.get_support(indices=True)]

        # í”¼ì²˜ê°€ ë„ˆë¬´ ë§ì´ ì œê±°ë˜ì—ˆë‹¤ë©´ ì›ë³¸ ì‚¬ìš©
        if len(selected_features_var) < 5:
            return X_train, X_test, feature_names, None

        # ìƒí˜¸ ì •ë³´ëŸ‰ ê¸°ë°˜ í”¼ì²˜ ì„ íƒ
        selector = SelectKBest(score_func=mutual_info_classif, k=min(k, len(selected_features_var)))
        selector.fit(X_train_var, y_train)

        # ì„ íƒëœ í”¼ì²˜ ì¸ë±ìŠ¤
        selected_indices = selector.get_support(indices=True)
        final_features = [selected_features_var[i] for i in selected_indices]

        # ë³€í™˜
        X_train_selected = selector.transform(X_train_var)
        X_test_selected = selector.transform(X_test_var)

        print(f"í”¼ì²˜ ì„ íƒ ì™„ë£Œ: {X_train.shape[1]} â†’ {X_train_selected.shape[1]}")

        return X_train_selected, X_test_selected, final_features, selector

    except Exception as e:
        print(f"í”¼ì²˜ ì„ íƒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return X_train, X_test, feature_names, None

def build_advanced_lstm_model(input_shape: Tuple[int, int]) -> keras.Model:
    """ê°œì„ ëœ LSTM ëª¨ë¸ êµ¬ì¶•"""
    if not TENSORFLOW_AVAILABLE:
        return None

    model = keras.Sequential([
        layers.LSTM(128, return_sequences=True, input_shape=input_shape),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.LSTM(64, return_sequences=True),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.LSTM(32, return_sequences=False),
        layers.BatchNormalization(),
        layers.Dropout(0.2),
        layers.Dense(16, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.2),
        layers.Dense(1, activation='sigmoid')
    ])

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy', 'AUC', keras.metrics.Precision(), keras.metrics.Recall()]
    )

    return model

def train_prophet_for_manager(manager_data: pd.DataFrame) -> Any:
    """Prophet ëª¨ë¸ í•™ìŠµ"""
    if not PROPHET_AVAILABLE:
        return None

    try:
        # Prophet í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì¤€ë¹„
        prophet_data = manager_data[['ë‚ ì§œ', 'ìŠ¹ì¸ìœ¨']].rename(
            columns={'ë‚ ì§œ': 'ds', 'ìŠ¹ì¸ìœ¨': 'y'}
        )

        # ëª¨ë¸ í•™ìŠµ
        model = Prophet(
            changepoint_prior_scale=0.05,
            seasonality_prior_scale=10,
            holidays_prior_scale=10,
            daily_seasonality=False,
            weekly_seasonality=True,
            yearly_seasonality=False
        )

        model.fit(prophet_data)
        return model

    except Exception as e:
        print(f"Prophet í•™ìŠµ ì˜¤ë¥˜: {str(e)}")
        return None

def train_prediction_model_with_data(filtered_data: pd.DataFrame = None) -> Tuple[Dict, str]:
    """ê°œì„ ëœ ë¦¬ë“œ ì „í™˜ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ"""
    try:
        if filtered_data is None:
            filtered_data = app_state.processed_data

        if filtered_data is None or len(filtered_data) == 0:
            return {}, "âŒ í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        data = filtered_data

        # íƒ€ê²Ÿ ë³€ìˆ˜ í™•ì¸
        if 'is_approved' not in data.columns:
            return {}, "âŒ íƒ€ê²Ÿ ë³€ìˆ˜(is_approved)ê°€ ì—†ìŠµë‹ˆë‹¤."

        # í”¼ì²˜ ì¤€ë¹„
        feature_cols = []

        # ìˆ˜ì¹˜í˜• í”¼ì²˜ (í™•ì¥ëœ ë²„ì „)
        numerical_features = [
            'ì ‘ì´‰íšŸìˆ˜', 'ë©”ëª¨_ê¸¸ì´', 'ë©”ëª¨_ë‹¨ì–´ìˆ˜',
            'ë“±ë¡_ì›”', 'ë“±ë¡_ì¼', 'ë“±ë¡_ìš”ì¼', 'ë“±ë¡_ì‹œê°„',
            'ë‹´ë‹¹ì_ìŠ¹ì¸ìœ¨', 'ë‹´ë‹¹ì_ì´ìŠ¹ì¸ìˆ˜', 'ë‹´ë‹¹ì_ì´ë¦¬ë“œìˆ˜',
            'ë‹´ë‹¹ì_í‰ê· ì ‘ì´‰', 'ë‹´ë‹¹ì_ê²½í—˜ì§€ìˆ˜', 'ë‹´ë‹¹ì_íš¨ìœ¨ì„±',
            'íŒë§¤ì²˜_í‰ê· ìŠ¹ì¸ìœ¨', 'íŒë§¤ì²˜_ë¹ˆë„', 'í”„ë¡œíŒŒì¼_ìŠ¹ì¸ìœ¨',
            'ë™ì˜ì ìˆ˜', 'ì™„ì „ë™ì˜', 'ë©”ëª¨_í’ˆì§ˆì ìˆ˜',
            'ë‹´ë‹¹ìXíŒë§¤ì²˜_ì‹œë„ˆì§€', 'ì ‘ì´‰Xë©”ëª¨_ìƒí˜¸ì‘ìš©'
        ]

        # ë²”ì£¼í˜• í”¼ì²˜
        categorical_features = ['ë‹´ë‹¹ìëª…', 'íŒë§¤ì²˜', 'ì„±ë³„', 'ìœ í˜•', 'ì‹œê°„ëŒ€', 'ê³ ê°í”„ë¡œíŒŒì¼']

        # ë™ì˜ ê´€ë ¨ í”¼ì²˜
        consent_features = ['ë§ˆì¼€íŒ…í™œìš©ë™ì˜_num', 'ì´ë©”ì¼ìˆ˜ì‹ ë™ì˜_num', 'ë¬¸ììˆ˜ì‹ ë™ì˜_num']

        # í‚¤ì›Œë“œ í”¼ì²˜
        keyword_features = [col for col in data.columns if col.startswith('ë©”ëª¨_') and col.endswith('_í¬í•¨')]

        # ì´ìƒì¹˜ í”¼ì²˜
        outlier_features = [col for col in data.columns if col.endswith('_ì´ìƒì¹˜')]

        # ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ë§Œ ì„ íƒ
        all_features = numerical_features + consent_features + keyword_features + outlier_features
        for col in all_features:
            if col in data.columns:
                feature_cols.append(col)

        if len(feature_cols) == 0:
            return {}, "âŒ ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
        encoded_data = data.copy()
        encoders = {}

        for col in categorical_features:
            if col in data.columns:
                try:
                    encoded_data[col] = encoded_data[col].fillna('Unknown')
                    le = LabelEncoder()
                    encoded_data[f'{col}_encoded'] = le.fit_transform(encoded_data[col].astype(str))
                    encoders[col] = le
                    feature_cols.append(f'{col}_encoded')
                except Exception as e:
                    print(f"âš ï¸ {col} ì¸ì½”ë”© ì‹¤íŒ¨: {str(e)}")

        # í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
        X = encoded_data[feature_cols].fillna(0)
        y = encoded_data['is_approved']

        # ë°ì´í„° ê²€ì¦
        if len(X) < 50:
            return {}, f"âŒ í•™ìŠµ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. (í˜„ì¬: {len(X)}ê±´, ìµœì†Œ: 50ê±´)"

        # ìŠ¤ì¼€ì¼ë§
        scaler = RobustScaler()  # ì´ìƒì¹˜ì— ê°•í•œ ìŠ¤ì¼€ì¼ëŸ¬
        X_scaled = scaler.fit_transform(X)
        app_state.scaler = scaler

        # ë°ì´í„° ë¶„í• 
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42, stratify=y
            )
        except:
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42
            )

        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
        X_train_balanced, y_train_balanced = handle_class_imbalance(X_train, y_train)

        # í”¼ì²˜ ì„ íƒ
        X_train_selected, X_test_selected, selected_features, feature_selector = select_best_features(
            X_train_balanced, y_train_balanced, X_test, feature_cols
        )
        app_state.feature_selector = feature_selector

        # ëª¨ë¸ ì •ì˜
        models = {
            'Random Forest': RandomForestClassifier(
                n_estimators=100, random_state=42, max_depth=10,
                min_samples_split=5, min_samples_leaf=2,
                class_weight='balanced'
            ),
            'Gradient Boosting': GradientBoostingClassifier(
                n_estimators=100, random_state=42, max_depth=5,
                learning_rate=0.1, subsample=0.8
            )
        }

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œë„ (ì„ íƒì )
        optimization_attempted = False
        if OPTUNA_AVAILABLE and len(X_train_selected) >= 100:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ
            try:
                print("í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œë„ ì¤‘...")

                # XGBoost ìµœì í™”
                xgb_params = optimize_hyperparameters(X_train_selected, y_train_balanced, 'xgboost')
                if xgb_params:
                    models['XGBoost (Optimized)'] = xgb.XGBClassifier(**xgb_params)
                    optimization_attempted = True

                # LightGBM ìµœì í™”
                lgb_params = optimize_hyperparameters(X_train_selected, y_train_balanced, 'lightgbm')
                if lgb_params:
                    models['LightGBM (Optimized)'] = lgb.LGBMClassifier(**lgb_params)
                    optimization_attempted = True

                # CatBoost ìµœì í™”
                if CATBOOST_AVAILABLE:
                    cat_params = optimize_hyperparameters(X_train_selected, y_train_balanced, 'catboost')
                    if cat_params:
                        models['CatBoost (Optimized)'] = CatBoostClassifier(**cat_params)
                        optimization_attempted = True

            except Exception as e:
                print(f"ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                optimization_attempted = False

        # ê¸°ë³¸ ëª¨ë¸ ì¶”ê°€ (ìµœì í™” ì‹¤íŒ¨ ì‹œ ë˜ëŠ” í•­ìƒ í¬í•¨)
        if not optimization_attempted or True:  # í•­ìƒ ê¸°ë³¸ ëª¨ë¸ë„ í¬í•¨
            models['XGBoost'] = xgb.XGBClassifier(
                n_estimators=100, random_state=42, max_depth=6,
                learning_rate=0.1, use_label_encoder=False, eval_metric='logloss'
            )
            models['LightGBM'] = lgb.LGBMClassifier(
                n_estimators=100, random_state=42, max_depth=6,
                learning_rate=0.1, verbose=-1
            )
            if CATBOOST_AVAILABLE:
                models['CatBoost'] = CatBoostClassifier(
                    iterations=100, random_state=42, depth=6,
                    learning_rate=0.1, verbose=False
                )

        # ì‹œê³„ì—´ ëª¨ë¸ ì¶”ê°€ (ì„ íƒì  - ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë§Œ)
        time_series_models = {}
        time_series_attempted = 0

        # ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„
        if 'ë“±ë¡ì¼ì' in data.columns:
            ts_data = prepare_time_series_data(data)

            if len(ts_data) > 0:
                # ë‹´ë‹¹ìë³„ ì‹œê³„ì—´ ëª¨ë¸ í•™ìŠµ
                for manager in ts_data['ë‹´ë‹¹ì'].unique()[:5]:  # ìƒìœ„ 5ëª…ë§Œ
                    manager_ts = ts_data[ts_data['ë‹´ë‹¹ì'] == manager]

                    # LSTM ëª¨ë¸ (TensorFlow í•„ìš”)
                    if TENSORFLOW_AVAILABLE and len(manager_ts) >= 30:
                        try:
                            lstm_model, lstm_scaler = train_lstm_for_manager(manager_ts)
                            if lstm_model:
                                time_series_models[f'LSTM_{manager}'] = {
                                    'model': lstm_model,
                                    'scaler': lstm_scaler,
                                    'type': 'lstm'
                                }
                                time_series_attempted += 1
                        except Exception as e:
                            print(f"LSTM ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨ ({manager}): {str(e)}")

                    # ARIMA ëª¨ë¸ (statsmodels í•„ìš”)
                    if STATSMODELS_AVAILABLE and len(manager_ts) >= 30:
                        try:
                            arima_model = train_arima_for_manager(manager_ts)
                            if arima_model:
                                time_series_models[f'ARIMA_{manager}'] = {
                                    'model': arima_model,
                                    'type': 'arima'
                                }
                                time_series_attempted += 1
                        except Exception as e:
                            print(f"ARIMA ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨ ({manager}): {str(e)}")

                    # Prophet ëª¨ë¸ (prophet í•„ìš”)
                    if PROPHET_AVAILABLE and len(manager_ts) >= 30:
                        try:
                            prophet_model = train_prophet_for_manager(manager_ts)
                            if prophet_model:
                                time_series_models[f'Prophet_{manager}'] = {
                                    'model': prophet_model,
                                    'type': 'prophet'
                                }
                                time_series_attempted += 1
                        except Exception as e:
                            print(f"Prophet ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨ ({manager}): {str(e)}")

        results = {}
        best_score = -1
        best_model = None
        best_model_name = None
        ensemble_models = []

        result_text = f"ğŸ¤– ê³ ê¸‰ ëª¨ë¸ í•™ìŠµ ê²°ê³¼ (í•™ìŠµ ë°ì´í„°: {len(X)}ê±´):\n" + "="*50 + "\n\n"

        # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
        result_text += "ğŸ“Š ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥:\n"
        for name, model in models.items():
            try:
                model.fit(X_train_selected, y_train_balanced)
                y_pred = model.predict(X_test_selected)
                y_pred_proba = model.predict_proba(X_test_selected)[:, 1]

                # ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ ê³„ì‚°
                roc_auc = roc_auc_score(y_test, y_pred_proba)
                f1 = f1_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred)
                recall = recall_score(y_test, y_pred)

                results[name] = {
                    'model': model,
                    'roc_auc': roc_auc,
                    'f1_score': f1,
                    'precision': precision,
                    'recall': recall,
                    'predictions': y_pred_proba,
                    'y_test': y_test,
                    'type': 'ml'
                }

                result_text += f"  - {name}:\n"
                result_text += f"    â€¢ ROC-AUC: {roc_auc:.4f}\n"
                result_text += f"    â€¢ F1-Score: {f1:.4f}\n"
                result_text += f"    â€¢ Precision: {precision:.4f}\n"
                result_text += f"    â€¢ Recall: {recall:.4f}\n"

                if roc_auc > best_score:
                    best_score = roc_auc
                    best_model = model
                    best_model_name = name

                # ì•™ìƒë¸”ì— í¬í•¨í•  ëª¨ë¸ ì„ íƒ (ROC-AUC > 0.6)
                if roc_auc > 0.6:
                    ensemble_models.append((name, model))

            except Exception as e:
                result_text += f"  âŒ {name} í•™ìŠµ ì‹¤íŒ¨: {str(e)}\n"

        # ì•™ìƒë¸” ëª¨ë¸ ìƒì„± (3ê°œ ì´ìƒì˜ ëª¨ë¸ì´ ì„±ê³µí•œ ê²½ìš°)
        if len(ensemble_models) >= 2:  # ê¸°ì¤€ì„ 3ê°œì—ì„œ 2ê°œë¡œ ë‚®ì¶¤
            try:
                voting_clf = VotingClassifier(
                    estimators=ensemble_models,
                    voting='soft'
                )
                voting_clf.fit(X_train_selected, y_train_balanced)
                y_pred_ensemble = voting_clf.predict_proba(X_test_selected)[:, 1]
                ensemble_auc = roc_auc_score(y_test, y_pred_ensemble)

                results['Ensemble'] = {
                    'model': voting_clf,
                    'roc_auc': ensemble_auc,
                    'predictions': y_pred_ensemble,
                    'y_test': y_test,
                    'type': 'ensemble'
                }

                result_text += f"\nğŸ¯ ì•™ìƒë¸” ëª¨ë¸ (ìƒìœ„ {len(ensemble_models)}ê°œ ê²°í•©):\n"
                result_text += f"  - ROC-AUC: {ensemble_auc:.4f}\n"

                if ensemble_auc > best_score:
                    best_score = ensemble_auc
                    best_model = voting_clf
                    best_model_name = 'Ensemble'
                    app_state.ensemble_model = voting_clf

            except Exception as e:
                result_text += f"\nâš ï¸ ì•™ìƒë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}\n"

        # ì‹œê³„ì—´ ëª¨ë¸ ê²°ê³¼ ì¶”ê°€
        if time_series_models:
            result_text += "\nğŸ“ˆ ì‹œê³„ì—´ ì „ë¬¸ ëª¨ë¸:\n"
            for name, model_info in time_series_models.items():
                result_text += f"  - {name}: í•™ìŠµ ì™„ë£Œ\n"
                results[name] = model_info

        if best_model is None:
            return {}, "âŒ ëª¨ë“  ëª¨ë¸ í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

        result_text += f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name} (ROC-AUC: {best_score:.4f})\n"

        # í”¼ì²˜ ì¤‘ìš”ë„
        if hasattr(best_model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                'feature': selected_features,
                'importance': best_model.feature_importances_
            }).sort_values('importance', ascending=False).head(10)

            result_text += "\nğŸ“Š ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì²˜:\n"
            for _, row in feature_importance.iterrows():
                result_text += f"  - {row['feature']}: {row['importance']:.4f}\n"

        # ì‹œê³„ì—´ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
        if time_series_models:
            result_text += f"\nğŸ”® ì‹œê³„ì—´ ëª¨ë¸ {len(time_series_models)}ê°œ ì¶”ê°€ í•™ìŠµ ì™„ë£Œ!"
            result_text += "\n   (ì‚¬ìš© ê°€ëŠ¥: "
            model_types = set([info['type'] for info in time_series_models.values()])
            result_text += ", ".join([t.upper() for t in model_types]) + ")"
        else:
            result_text += "\nğŸ“Š ì‹œê³„ì—´ ëª¨ë¸ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ì§€ë§Œ, ML ëª¨ë¸ë¡œ ì¶©ë¶„í•œ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."

        result_text += "\n\nâœ… ê³ ê¸‰ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!"

        # ìƒíƒœ ì €ì¥
        app_state.best_model = best_model
        app_state.best_model_name = best_model_name
        app_state.feature_names = selected_features
        app_state.encoders = encoders
        app_state.model_results = results
        app_state.time_series_models = time_series_models if time_series_models else {}
        app_state.is_model_trained = True

        # ë‹´ë‹¹ìë³„ ì„±ê³¼ ì¬ê³„ì‚° (í•„í„°ë§ëœ ë°ì´í„° ê¸°ì¤€)
        manager_stats = data[data['ë‹´ë‹¹ìëª…'] != 'ë¯¸ë°°ì •'].groupby('ë‹´ë‹¹ìëª…').agg({
            'is_approved': ['count', 'sum', 'mean'],
            'ì ‘ì´‰íšŸìˆ˜': 'mean'
        }).round(3)

        if len(manager_stats) > 0:
            manager_stats.columns = ['ì´_ë¦¬ë“œìˆ˜', 'ìŠ¹ì¸ì™„ë£Œìˆ˜', 'ìŠ¹ì¸ì™„ë£Œìœ¨', 'í‰ê· _ì ‘ì´‰íšŸìˆ˜']
            manager_stats = manager_stats.reset_index()
            app_state.manager_performance = manager_stats

        return results, result_text

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        error_message = f"âŒ ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:\n{str(e)}\n\n"

        # ë” ìì„¸í•œ ì˜¤ë¥˜ ì •ë³´ ì œê³µ
        if "No trials are completed" in str(e):
            error_message += "ğŸ’¡ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ë¡œ ì¬ì‹œë„ ì¤‘...\n"

            # ê¸°ë³¸ ëª¨ë¸ë¡œ ì¬ì‹œë„
            try:
                # ê°„ë‹¨í•œ ëª¨ë¸ë¡œ ì¬ì‹œë„
                simple_model = RandomForestClassifier(n_estimators=50, random_state=42, max_depth=5)
                simple_model.fit(X_train[:, :min(10, X_train.shape[1])], y_train)  # í”¼ì²˜ ìˆ˜ ì œí•œ

                app_state.best_model = simple_model
                app_state.best_model_name = "Random Forest (Basic)"
                app_state.is_model_trained = True

                return {"Random Forest (Basic)": {'model': simple_model}}, "âœ… ê¸°ë³¸ ëª¨ë¸ë¡œ í•™ìŠµ ì™„ë£Œ!"

            except Exception as e2:
                error_message += f"\nê¸°ë³¸ ëª¨ë¸ í•™ìŠµë„ ì‹¤íŒ¨: {str(e2)}"

        return {}, error_message

# ================================================================================
# 10. ë‚˜ë¨¸ì§€ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)
# ================================================================================

# ê¸°ì¡´ ì½”ë“œì˜ ë‚˜ë¨¸ì§€ í•¨ìˆ˜ë“¤ì„ ê·¸ëŒ€ë¡œ í¬í•¨
# - prepare_time_series_data
# - build_lstm_model (advanced ë²„ì „ìœ¼ë¡œ ëŒ€ì²´ë¨)
# - train_lstm_for_manager
# - train_arima_for_manager
# - filter_data_by_date
# - create_time_series_prediction
# - train_prediction_model
# - predict_manager_performance
# - optimize_lead_assignment
# - create_gradio_interface

def prepare_time_series_data(data: pd.DataFrame) -> pd.DataFrame:
    """ì‹œê³„ì—´ ëª¨ë¸ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
    try:
        if 'ë“±ë¡ì¼ì' not in data.columns:
            return pd.DataFrame()

        # ì¼ë³„ ìŠ¹ì¸ìœ¨ ì§‘ê³„
        daily_stats = data.groupby([pd.Grouper(key='ë“±ë¡ì¼ì', freq='D'), 'ë‹´ë‹¹ìëª…']).agg({
            'is_approved': ['count', 'sum', 'mean']
        }).reset_index()

        daily_stats.columns = ['ë‚ ì§œ', 'ë‹´ë‹¹ì', 'ë¦¬ë“œìˆ˜', 'ìŠ¹ì¸ìˆ˜', 'ìŠ¹ì¸ìœ¨']
        daily_stats = daily_stats[daily_stats['ë¦¬ë“œìˆ˜'] > 0]  # ë¦¬ë“œê°€ ìˆëŠ” ë‚ ë§Œ

        # ì‹œê³„ì—´ í”¼ì²˜ ì¶”ê°€
        daily_stats['ìš”ì¼'] = daily_stats['ë‚ ì§œ'].dt.dayofweek
        daily_stats['ì›”'] = daily_stats['ë‚ ì§œ'].dt.month
        daily_stats['ë¶„ê¸°'] = daily_stats['ë‚ ì§œ'].dt.quarter

        return daily_stats

    except Exception as e:
        print(f"ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„ ì˜¤ë¥˜: {str(e)}")
        return pd.DataFrame()

def train_lstm_for_manager(manager_data: pd.DataFrame, sequence_length: int = 7) -> Tuple[keras.Model, StandardScaler]:
    """ë‹´ë‹¹ìë³„ LSTM ëª¨ë¸ í•™ìŠµ (ê°œì„ ëœ ë²„ì „)"""
    if not TENSORFLOW_AVAILABLE:
        return None, None

    try:
        # ë°ì´í„° ì¤€ë¹„
        data = manager_data.sort_values('ë‚ ì§œ')

        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        features = ['ë¦¬ë“œìˆ˜', 'ìŠ¹ì¸ìˆ˜', 'ìš”ì¼', 'ì›”']
        X = data[features].values
        y = data['ìŠ¹ì¸ìœ¨'].values

        # ì •ê·œí™”
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # ì‹œí€€ìŠ¤ ìƒì„±
        X_seq, y_seq = [], []
        for i in range(len(X_scaled) - sequence_length):
            X_seq.append(X_scaled[i:i+sequence_length])
            y_seq.append(y[i+sequence_length])

        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq)

        if len(X_seq) < 10:  # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ í•™ìŠµ ë¶ˆê°€
            return None, None

        # ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ
        model = build_advanced_lstm_model((sequence_length, len(features)))

        # ì½œë°± ì„¤ì •
        early_stop = EarlyStopping(
            monitor='loss',
            patience=20,
            restore_best_weights=True
        )

        reduce_lr = ReduceLROnPlateau(
            monitor='loss',
            factor=0.5,
            patience=10,
            min_lr=0.0001
        )

        # í•™ìŠµ
        model.fit(
            X_seq, y_seq,
            epochs=100,
            batch_size=16,
            verbose=0,
            callbacks=[early_stop, reduce_lr],
            validation_split=0.2
        )

        return model, scaler

    except Exception as e:
        print(f"LSTM í•™ìŠµ ì˜¤ë¥˜: {str(e)}")
        return None, None

def train_arima_for_manager(manager_data: pd.DataFrame) -> Any:
    """ë‹´ë‹¹ìë³„ ARIMA ëª¨ë¸ í•™ìŠµ"""
    if not STATSMODELS_AVAILABLE:
        return None

    try:
        # ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„
        ts_data = manager_data.set_index('ë‚ ì§œ')['ìŠ¹ì¸ìœ¨']
        ts_data = ts_data.asfreq('D').ffill()  # ì¼ë³„ ì£¼ê¸°ë¡œ ë³€í™˜

        if len(ts_data) < 30:  # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ í•™ìŠµ ë¶ˆê°€
            return None

        # Auto ARIMAë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°
        try:
            model = pm.auto_arima(
                ts_data,
                seasonal=True,
                m=7,  # ì£¼ê°„ ê³„ì ˆì„±
                stepwise=True,
                suppress_warnings=True,
                error_action='ignore',
                max_p=3,
                max_q=3,
                max_order=5,
                trace=False,  # ì§„í–‰ ìƒí™© ì¶œë ¥ ì•ˆí•¨
                n_jobs=1  # ë‹¨ì¼ ìŠ¤ë ˆë“œ ì‚¬ìš©
            )
        except:
            # Auto ARIMA ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ ARIMA ì‹œë„
            from statsmodels.tsa.arima.model import ARIMA
            model = ARIMA(ts_data, order=(1,1,1))
            model = model.fit()

        return model

    except Exception as e:
        print(f"ARIMA í•™ìŠµ ì˜¤ë¥˜ ({manager_data['ë‹´ë‹¹ì'].iloc[0] if len(manager_data) > 0 else 'ì•Œ ìˆ˜ ì—†ìŒ'}): {str(e)}")
        return None

def filter_data_by_date(start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:
    """ì§€ì •ëœ ë‚ ì§œ ë²”ìœ„ì˜ ë°ì´í„°ë§Œ í•„í„°ë§"""
    try:
        if app_state.processed_data is None:
            return None

        data = app_state.processed_data.copy()

        # ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°ë§Œ í•„í„°ë§
        if 'ë“±ë¡ì¼ì' in data.columns:
            # ë‚ ì§œ í˜•ì‹ í™•ì¸ ë° ë³€í™˜
            if not pd.api.types.is_datetime64_any_dtype(data['ë“±ë¡ì¼ì']):
                data['ë“±ë¡ì¼ì'] = pd.to_datetime(data['ë“±ë¡ì¼ì'], errors='coerce')

            # ë‚ ì§œ ë²”ìœ„ë¡œ í•„í„°ë§
            mask = (data['ë“±ë¡ì¼ì'] >= start_date) & (data['ë“±ë¡ì¼ì'] <= end_date)
            filtered_data = data[mask]

            print(f"ë‚ ì§œ í•„í„°ë§: {start_date.date()} ~ {end_date.date()}")
            print(f"í•„í„°ë§ ì „: {len(data)}ê±´ â†’ í•„í„°ë§ í›„: {len(filtered_data)}ê±´")

            return filtered_data
        else:
            # ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ë°˜í™˜
            return data

    except Exception as e:
        print(f"ë‚ ì§œ í•„í„°ë§ ì˜¤ë¥˜: {str(e)}")
        return None

def create_time_series_prediction(start_date: pd.Timestamp, days: int) -> go.Figure:
    """ì‹œê³„ì—´ ì˜ˆì¸¡ ê·¸ë˜í”„ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
    try:
        if not app_state.is_model_trained or app_state.manager_performance is None:
            return None

        # ì˜ˆì¸¡ ë‚ ì§œ ë²”ìœ„ ìƒì„±
        date_range = pd.date_range(start=start_date, periods=days, freq='D')

        # ë‹´ë‹¹ìë³„ ì˜ˆì¸¡ ë°ì´í„° ìƒì„±
        predictions_by_date = []

        # ê° ë‹´ë‹¹ìì˜ ì¼ì¼ í‰ê·  ë¦¬ë“œ ìˆ˜ ê³„ì‚°
        manager_daily_leads = {}
        for _, manager_row in app_state.manager_performance.head(5).iterrows():
            manager = manager_row['ë‹´ë‹¹ìëª…']
            daily_avg = manager_row['ì´_ë¦¬ë“œìˆ˜'] / 30
            manager_daily_leads[manager] = daily_avg

        # ì‹œê³„ì—´ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        use_time_series = len(app_state.time_series_models) > 0

        for date in date_range:
            for _, manager_row in app_state.manager_performance.head(5).iterrows():
                manager = manager_row['ë‹´ë‹¹ìëª…']

                # ì‹œê³„ì—´ ëª¨ë¸ ìš°ì„  ì‚¬ìš©
                pred_proba = None
                model_used = "ML"
                confidence = 0.8  # ê¸°ë³¸ ì‹ ë¢°ë„

                # Prophet ëª¨ë¸ í™•ì¸ (ìµœìš°ì„ )
                prophet_key = f'Prophet_{manager}'
                if use_time_series and prophet_key in app_state.time_series_models:
                    try:
                        prophet_model = app_state.time_series_models[prophet_key]['model']
                        future = prophet_model.make_future_dataframe(periods=1)
                        forecast = prophet_model.predict(future)
                        pred_proba = forecast['yhat'].iloc[-1]
                        pred_proba = max(0, min(1, pred_proba))
                        model_used = "Prophet"
                        confidence = 0.9
                    except:
                        pass

                # LSTM ëª¨ë¸ í™•ì¸
                lstm_key = f'LSTM_{manager}'
                if pred_proba is None and use_time_series and lstm_key in app_state.time_series_models:
                    try:
                        lstm_info = app_state.time_series_models[lstm_key]
                        lstm_model = lstm_info['model']
                        lstm_scaler = lstm_info['scaler']

                        # ìµœê·¼ 7ì¼ ë°ì´í„° ì¤€ë¹„ (ì‹œë®¬ë ˆì´ì…˜)
                        recent_features = np.array([
                            [manager_daily_leads[manager],
                             manager_daily_leads[manager] * manager_row['ìŠ¹ì¸ì™„ë£Œìœ¨'],
                             date.dayofweek,
                             date.month]
                        ])

                        recent_scaled = lstm_scaler.transform(recent_features)
                        sequence = np.tile(recent_scaled, (7, 1)).reshape(1, 7, -1)

                        pred_proba = lstm_model.predict(sequence)[0, 0]
                        model_used = "LSTM"
                        confidence = 0.85
                    except:
                        pass

                # ARIMA ëª¨ë¸ í™•ì¸
                arima_key = f'ARIMA_{manager}'
                if pred_proba is None and use_time_series and arima_key in app_state.time_series_models:
                    try:
                        arima_model = app_state.time_series_models[arima_key]['model']
                        # ARIMA ì˜ˆì¸¡ (1ì¼ ì•)
                        forecast = arima_model.forecast(steps=1)
                        pred_proba = forecast[0]
                        pred_proba = max(0, min(1, pred_proba))  # 0-1 ë²”ìœ„ë¡œ ì œí•œ
                        model_used = "ARIMA"
                        confidence = 0.8
                    except:
                        pass

                # ê¸°ë³¸ ML ëª¨ë¸ ì‚¬ìš©
                if pred_proba is None:
                    # ë” ë§ì€ í”¼ì²˜ ì‚¬ìš©
                    sample_data = app_state.processed_data[
                        app_state.processed_data['ë‹´ë‹¹ìëª…'] == manager
                    ].mean(numeric_only=True).to_dict()

                    # ë‚ ì§œ ê´€ë ¨ í”¼ì²˜ ì—…ë°ì´íŠ¸
                    sample_data.update({
                        'ë“±ë¡_ì›”': date.month,
                        'ë“±ë¡_ì¼': date.day,
                        'ë“±ë¡_ìš”ì¼': date.dayofweek,
                        'ë“±ë¡_ì‹œê°„': 14,
                        'ë‹´ë‹¹ìëª…': manager
                    })

                    sample_df = pd.DataFrame([sample_data])

                    # ì¸ì½”ë”©
                    for col, encoder in app_state.encoders.items():
                        if col in sample_df.columns:
                            try:
                                sample_df[f'{col}_encoded'] = encoder.transform(sample_df[col])
                            except:
                                sample_df[f'{col}_encoded'] = 0

                    # í”¼ì²˜ ì„ íƒ ë° ìŠ¤ì¼€ì¼ë§
                    try:
                        X_pred = sample_df[app_state.feature_names].fillna(0)
                        if app_state.scaler:
                            X_pred = app_state.scaler.transform(X_pred)
                        if app_state.feature_selector:
                            X_pred = app_state.feature_selector.transform(X_pred)

                        pred_proba = app_state.best_model.predict_proba(X_pred)[0, 1]
                        confidence = 0.7
                    except:
                        pred_proba = manager_row['ìŠ¹ì¸ì™„ë£Œìœ¨']
                        confidence = 0.6

                # ì˜ˆìƒ ìŠ¹ì¸ìˆ˜ ê³„ì‚°
                daily_leads = manager_daily_leads[manager]
                expected_approvals = daily_leads * pred_proba

                # ì‹ ë¢° êµ¬ê°„ ê³„ì‚°
                std_dev = expected_approvals * 0.1 * (1 - confidence)
                lower_bound = max(0, expected_approvals - 1.96 * std_dev)
                upper_bound = expected_approvals + 1.96 * std_dev

                predictions_by_date.append({
                    'ë‚ ì§œ': date,
                    'ë‹´ë‹¹ì': manager,
                    'ì˜ˆìƒ_ìŠ¹ì¸ìˆ˜': expected_approvals,
                    'ì˜ˆìƒ_ìŠ¹ì¸ìœ¨': pred_proba,
                    'ì˜ˆìƒ_ë¦¬ë“œìˆ˜': daily_leads,
                    'ëª¨ë¸': model_used,
                    'ì‹ ë¢°ë„': confidence,
                    'í•˜í•œ': lower_bound,
                    'ìƒí•œ': upper_bound
                })

        # DataFrame ë³€í™˜
        pred_df = pd.DataFrame(predictions_by_date)

        # ì‹œê³„ì—´ ê·¸ë˜í”„ ìƒì„±
        fig = make_subplots(
            rows=4, cols=1,
            subplot_titles=(
                f'{days}ì¼ê°„ ë‹´ë‹¹ìë³„ ì˜ˆìƒ ìŠ¹ì¸ìˆ˜ (95% ì‹ ë¢°êµ¬ê°„)',
                'ì¼ë³„ ì „ì²´ ì˜ˆìƒ ìŠ¹ì¸ìˆ˜',
                'ëª¨ë¸ë³„ ì˜ˆì¸¡ ê¸°ì—¬ë„',
                'ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¶„í¬'
            ),
            row_heights=[0.4, 0.25, 0.15, 0.2],
            vertical_spacing=0.12
        )

        # 1. ë‹´ë‹¹ìë³„ ì˜ˆìƒ ìŠ¹ì¸ìˆ˜ ì¶”ì´ (ì‹ ë¢°êµ¬ê°„ í¬í•¨)
        for manager in pred_df['ë‹´ë‹¹ì'].unique():
            manager_data = pred_df[pred_df['ë‹´ë‹¹ì'] == manager]

            # ì£¼ ì¶”ì„¸ì„ 
            fig.add_trace(
                go.Scatter(
                    x=manager_data['ë‚ ì§œ'],
                    y=manager_data['ì˜ˆìƒ_ìŠ¹ì¸ìˆ˜'],
                    name=f"{manager}",
                    mode='lines+markers',
                    hovertemplate='%{y:.1f}ê±´<br>ìŠ¹ì¸ìœ¨: %{customdata:.1%}',
                    customdata=manager_data['ì˜ˆìƒ_ìŠ¹ì¸ìœ¨']
                ),
                row=1, col=1
            )

            # ì‹ ë¢°êµ¬ê°„
            fig.add_trace(
                go.Scatter(
                    x=manager_data['ë‚ ì§œ'].tolist() + manager_data['ë‚ ì§œ'].tolist()[::-1],
                    y=manager_data['ìƒí•œ'].tolist() + manager_data['í•˜í•œ'].tolist()[::-1],
                    fill='toself',
                    fillcolor=f'rgba(0,100,200,0.1)',
                    line=dict(color='rgba(255,255,255,0)'),
                    hoverinfo="skip",
                    showlegend=False,
                    name=f'{manager}_ì‹ ë¢°êµ¬ê°„'
                ),
                row=1, col=1
            )

        # 2. ì „ì²´ ì˜ˆìƒ ìŠ¹ì¸ìˆ˜
        daily_total = pred_df.groupby('ë‚ ì§œ').agg({
            'ì˜ˆìƒ_ìŠ¹ì¸ìˆ˜': 'sum',
            'í•˜í•œ': 'sum',
            'ìƒí•œ': 'sum'
        }).reset_index()

        fig.add_trace(
            go.Bar(
                x=daily_total['ë‚ ì§œ'],
                y=daily_total['ì˜ˆìƒ_ìŠ¹ì¸ìˆ˜'],
                name='ì „ì²´ ìŠ¹ì¸ìˆ˜',
                marker_color='lightblue',
                hovertemplate='%{y:.0f}ê±´',
                error_y=dict(
                    type='data',
                    symmetric=False,
                    array=daily_total['ìƒí•œ'] - daily_total['ì˜ˆìƒ_ìŠ¹ì¸ìˆ˜'],
                    arrayminus=daily_total['ì˜ˆìƒ_ìŠ¹ì¸ìˆ˜'] - daily_total['í•˜í•œ']
                ),
                showlegend=False
            ),
            row=2, col=1
        )

        # 3. ëª¨ë¸ë³„ ì‚¬ìš© ë¹„ìœ¨
        model_counts = pred_df['ëª¨ë¸'].value_counts()

        fig.add_trace(
            go.Bar(
                x=model_counts.index,
                y=model_counts.values,
                name='ëª¨ë¸ ì‚¬ìš© íšŸìˆ˜',
                marker_color=['darkgreen' if 'Prophet' in x else
                             'green' if 'LSTM' in x else
                             'orange' if 'ARIMA' in x else
                             'blue'
                             for x in model_counts.index],
                showlegend=False
            ),
            row=3, col=1
        )

        # 4. ì‹ ë¢°ë„ ë¶„í¬
        confidence_by_model = pred_df.groupby('ëª¨ë¸')['ì‹ ë¢°ë„'].mean().sort_values(ascending=False)

        fig.add_trace(
            go.Bar(
                x=confidence_by_model.index,
                y=confidence_by_model.values,
                name='í‰ê·  ì‹ ë¢°ë„',
                marker_color='lightcoral',
                text=confidence_by_model.values.round(2),
                textposition='outside',
                showlegend=False
            ),
            row=4, col=1
        )

        # ë ˆì´ì•„ì›ƒ ì„¤ì •
        fig.update_layout(
            height=1000,
            showlegend=True,
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            ),
            title_text=f"ê³ ê¸‰ AI ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼ (Prophet/LSTM/ARIMA/ML ì•™ìƒë¸”)"
        )

        # Yì¶• ì„¤ì •
        fig.update_yaxes(title_text="ì˜ˆìƒ ìŠ¹ì¸ìˆ˜ (ê±´)", row=1, col=1)
        fig.update_yaxes(title_text="ì „ì²´ ìŠ¹ì¸ìˆ˜ (ê±´)", row=2, col=1)
        fig.update_yaxes(title_text="ì˜ˆì¸¡ íšŸìˆ˜", row=3, col=1)
        fig.update_yaxes(title_text="ì‹ ë¢°ë„", row=4, col=1)
        fig.update_xaxes(title_text="ë‚ ì§œ", row=2, col=1)

        # ì£¼ë§ êµ¬ë¶„ì„  ì¶”ê°€
        for date in date_range:
            if date.dayofweek in [5, 6]:
                for row in [1, 2]:
                    fig.add_vrect(
                        x0=date,
                        x1=date + pd.Timedelta(days=1),
                        fillcolor="LightGray",
                        opacity=0.2,
                        layer="below",
                        line_width=0,
                        row=row, col=1
                    )

        # í‰ê· ì„  ì¶”ê°€
        avg_total = daily_total['ì˜ˆìƒ_ìŠ¹ì¸ìˆ˜'].mean()
        fig.add_hline(
            y=avg_total,
            line_dash="dash",
            line_color="red",
            annotation_text=f"ì¼ í‰ê· : {avg_total:.0f}ê±´",
            row=2, col=1
        )

        # ì˜ˆì¸¡ ìš”ì•½ ì •ë³´
        total_expected = daily_total['ì˜ˆìƒ_ìŠ¹ì¸ìˆ˜'].sum()
        model_usage = ", ".join([f"{model}: {count}íšŒ" for model, count in model_counts.items()])
        avg_confidence = pred_df['ì‹ ë¢°ë„'].mean()

        fig.add_annotation(
            text=f"<b>ì˜ˆì¸¡ ê¸°ê°„ ì´ ì˜ˆìƒ ìŠ¹ì¸ìˆ˜: {total_expected:,.0f}ê±´</b><br>" +
                 f"ëª¨ë¸ ì‚¬ìš©: {model_usage}<br>" +
                 f"í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„: {avg_confidence:.1%}",
            xref="paper", yref="paper",
            x=0.5, y=-0.15,
            showarrow=False,
            font=dict(size=14)
        )

        return fig

    except Exception as e:
        print(f"ì‹œê³„ì—´ ì˜ˆì¸¡ ê·¸ë˜í”„ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return None

def train_prediction_model() -> Tuple[Dict, str]:
    """ë¦¬ë“œ ì „í™˜ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ (ì‹œê³„ì—´ ëª¨ë¸ í¬í•¨)"""
    try:
        if not app_state.is_preprocessed or app_state.processed_data is None:
            return {}, "âŒ ë¨¼ì € ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”."

        # train_prediction_model_with_data í•¨ìˆ˜ í˜¸ì¶œ
        return train_prediction_model_with_data(app_state.processed_data)

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return {}, f"âŒ ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:\n{str(e)}\n\nìƒì„¸ ì˜¤ë¥˜:\n{error_details}"

def predict_manager_performance(prediction_date: pd.Timestamp = None, prediction_days: int = 30) -> pd.DataFrame:
    """ë‹´ë‹¹ìë³„ ì˜ˆìƒ ìŠ¹ì¸ìœ¨ ì˜ˆì¸¡ (ë‚ ì§œ ê¸°ë°˜)"""
    try:
        if not app_state.is_model_trained or app_state.best_model is None:
            return pd.DataFrame()

        if app_state.manager_performance is None or len(app_state.manager_performance) == 0:
            return pd.DataFrame()

        # ì˜ˆì¸¡ ë‚ ì§œ ì„¤ì •
        if prediction_date is None:
            prediction_date = pd.Timestamp.now()

        predictions = []

        # ê° ë‹´ë‹¹ìë³„ë¡œ ì˜ˆì¸¡
        for _, manager_row in app_state.manager_performance.iterrows():
            manager = manager_row['ë‹´ë‹¹ìëª…']

            if manager == 'ë¯¸ë°°ì •':
                continue

            # ë‹´ë‹¹ìì˜ í‰ê·  íŠ¹ì„± ì‚¬ìš©
            manager_data = app_state.processed_data[app_state.processed_data['ë‹´ë‹¹ìëª…'] == manager]

            if len(manager_data) == 0:
                continue

            # ì˜ˆì¸¡ ê¸°ê°„ ë™ì•ˆì˜ í‰ê·  ìŠ¹ì¸ìœ¨ ê³„ì‚°
            daily_predictions = []

            for days_ahead in range(0, prediction_days, 7):  # ì£¼ ë‹¨ìœ„ë¡œ ìƒ˜í”Œë§
                future_date = prediction_date + pd.Timedelta(days=days_ahead)

                # ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„ (ë” ë§ì€ í”¼ì²˜ ì‚¬ìš©)
                sample_data = manager_data.mean(numeric_only=True).to_dict()

                # ë‚ ì§œ ê´€ë ¨ í”¼ì²˜ ì—…ë°ì´íŠ¸
                sample_data.update({
                    'ë“±ë¡_ì›”': future_date.month,
                    'ë“±ë¡_ì¼': future_date.day,
                    'ë“±ë¡_ìš”ì¼': future_date.dayofweek,
                    'ë“±ë¡_ì‹œê°„': 14,
                    'ë‹´ë‹¹ìëª…': manager
                })

                # DataFrameìœ¼ë¡œ ë³€í™˜
                sample_df = pd.DataFrame([sample_data])

                # ì¸ì½”ë”©
                for col, encoder in app_state.encoders.items():
                    if col in sample_df.columns:
                        try:
                            sample_df[f'{col}_encoded'] = encoder.transform(sample_df[col])
                        except:
                            sample_df[f'{col}_encoded'] = 0

                # í”¼ì²˜ ì„ íƒ ë° ìŠ¤ì¼€ì¼ë§
                try:
                    X_pred = sample_df[app_state.feature_names].fillna(0)
                    if app_state.scaler:
                        X_pred = app_state.scaler.transform(X_pred)
                    if app_state.feature_selector:
                        X_pred = app_state.feature_selector.transform(X_pred)

                    pred_proba = app_state.best_model.predict_proba(X_pred)[0, 1]
                    daily_predictions.append(pred_proba)
                except:
                    daily_predictions.append(0.5)

            # í‰ê·  ì˜ˆì¸¡ ìŠ¹ì¸ìœ¨ ê³„ì‚°
            avg_pred_proba = np.mean(daily_predictions) if daily_predictions else 0.5

            # í˜„ì¬ ìŠ¹ì¸ê±´ìˆ˜ ê³„ì‚°
            current_approval_count = int(manager_row['ìŠ¹ì¸ì™„ë£Œìˆ˜'])
            total_leads = int(manager_row['ì´_ë¦¬ë“œìˆ˜'])

            # ì˜ˆì¸¡ ê¸°ê°„ ë™ì•ˆì˜ ì˜ˆìƒ ë¦¬ë“œìˆ˜ (ì¼ì¼ í‰ê·  * ì˜ˆì¸¡ ì¼ìˆ˜)
            daily_avg_leads = total_leads / 30  # í•œ ë‹¬ ê¸°ì¤€
            expected_leads = int(daily_avg_leads * prediction_days)

            # ì˜ˆì¸¡ ìŠ¹ì¸ê±´ìˆ˜ ê³„ì‚°
            predicted_approval_count = int(avg_pred_proba * expected_leads)

            predictions.append({
                'ë‹´ë‹¹ì': manager,
                'ì´ë¦¬ë“œìˆ˜': total_leads,
                'í‰ê· ì ‘ì´‰íšŸìˆ˜': round(manager_row['í‰ê· _ì ‘ì´‰íšŸìˆ˜'], 1),
                'í˜„ì¬ ìŠ¹ì¸ê±´': current_approval_count,
                'í˜„ì¬ ìŠ¹ì¸ìœ¨': f"{manager_row['ìŠ¹ì¸ì™„ë£Œìœ¨']:.1%}",
                'ì˜ˆì¸¡ ìŠ¹ì¸ê±´': predicted_approval_count,
                'ì˜ˆì¸¡ ìŠ¹ì¸ìœ¨': f"{avg_pred_proba:.1%}",
                'ì˜ˆì¸¡ ì •í™•ë„': f"{(1 - abs(avg_pred_proba - manager_row['ìŠ¹ì¸ì™„ë£Œìœ¨'])):.1%}"
            })

        if len(predictions) == 0:
            return pd.DataFrame()

        # ê²°ê³¼ ì •ë¦¬
        result_df = pd.DataFrame(predictions)

        # ì˜ˆì¸¡ ìŠ¹ì¸ìœ¨ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        result_df['sort_key'] = result_df['ì˜ˆì¸¡ ìŠ¹ì¸ìœ¨'].str.rstrip('%').astype(float)
        result_df = result_df.sort_values('sort_key', ascending=False).drop('sort_key', axis=1)

        # ì˜ˆì¸¡ ê¸°ê°„ ì •ë³´ ì¶”ê°€ (ì²« í–‰ì—ë§Œ í‘œì‹œ)
        if len(result_df) > 0:
            result_df.loc[result_df.index[0], 'ì˜ˆì¸¡ê¸°ê°„'] = f"{prediction_date.strftime('%Y-%m-%d')} ~ {(prediction_date + pd.Timedelta(days=prediction_days)).strftime('%Y-%m-%d')}"
            # ë‚˜ë¨¸ì§€ í–‰ì€ ë¹ˆ ë¬¸ìì—´
            result_df.loc[result_df.index[1:], 'ì˜ˆì¸¡ê¸°ê°„'] = ''

        return result_df

    except Exception as e:
        print(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return pd.DataFrame()

# ================================================================================
# 11. ë¦¬ë“œ ë°°ì • ìµœì í™” (ê°œì„ ëœ ë²„ì „)
# ================================================================================

def optimize_lead_assignment(channel, gender, type_, memo_len, marketing, email, sms) -> Tuple[str, str, go.Figure]:
    """ì‹ ê·œ ë¦¬ë“œì— ëŒ€í•œ ìµœì  ë‹´ë‹¹ì ì¶”ì²œ (ê°œì„ ëœ ë²„ì „)"""
    try:
        if not app_state.is_model_trained or app_state.best_model is None:
            return "", "âŒ ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.", None

        if app_state.manager_performance is None or len(app_state.manager_performance) == 0:
            return "", "âŒ ë‹´ë‹¹ì ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", None

        # ì‹ ê·œ ë¦¬ë“œ ì •ë³´ (í™•ì¥ëœ í”¼ì²˜)
        current_datetime = datetime.now()

        # ê¸°ë³¸ í”¼ì²˜
        new_lead = {
            'íŒë§¤ì²˜': channel,
            'ì„±ë³„': gender,
            'ìœ í˜•': type_,
            'ë©”ëª¨_ê¸¸ì´': memo_len,
            'ë©”ëª¨_ë‹¨ì–´ìˆ˜': memo_len // 5,
            'ë§ˆì¼€íŒ…í™œìš©ë™ì˜_num': int(marketing),
            'ì´ë©”ì¼ìˆ˜ì‹ ë™ì˜_num': int(email),
            'ë¬¸ììˆ˜ì‹ ë™ì˜_num': int(sms),
            'ì ‘ì´‰íšŸìˆ˜': 0,
            'ë“±ë¡_ì›”': current_datetime.month,
            'ë“±ë¡_ì¼': current_datetime.day,
            'ë“±ë¡_ìš”ì¼': current_datetime.weekday(),
            'ë“±ë¡_ì‹œê°„': current_datetime.hour,
            'ë™ì˜ì ìˆ˜': int(marketing) + int(email) + int(sms),
            'ì™„ì „ë™ì˜': int(marketing and email and sms),
            'ì£¼ë§ì—¬ë¶€': int(current_datetime.weekday() >= 5),
            'ì›”ì´ˆ': int(current_datetime.day <= 10),
            'ì›”ë§': int(current_datetime.day >= 21),
            'ë¶„ê¸°': (current_datetime.month - 1) // 3 + 1
        }

        # ë©”ëª¨ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        new_lead['ë©”ëª¨_í’ˆì§ˆì ìˆ˜'] = (
            new_lead['ë©”ëª¨_ê¸¸ì´'] / 500 * 0.3 +  # ìµœëŒ€ 500ì ê¸°ì¤€
            new_lead['ë©”ëª¨_ë‹¨ì–´ìˆ˜'] / 100 * 0.3 +  # ìµœëŒ€ 100ë‹¨ì–´ ê¸°ì¤€
            0.4  # í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ (ê°€ì •)
        )

        # ê³ ê° í”„ë¡œíŒŒì¼
        new_lead['ê³ ê°í”„ë¡œíŒŒì¼'] = f"{gender}_{type_}"

        # ê° ë‹´ë‹¹ìë³„ ì˜ˆìƒ ìŠ¹ì¸í™•ë¥  ê³„ì‚°
        predictions = []

        for _, manager_row in app_state.manager_performance.head(10).iterrows():
            manager = manager_row['ë‹´ë‹¹ìëª…']

            if manager == 'ë¯¸ë°°ì •':
                continue

            # ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„
            pred_data = new_lead.copy()
            pred_data['ë‹´ë‹¹ìëª…'] = manager

            # ë‹´ë‹¹ì ê´€ë ¨ í”¼ì²˜ ì¶”ê°€
            pred_data['ë‹´ë‹¹ì_ìŠ¹ì¸ìœ¨'] = manager_row['ìŠ¹ì¸ì™„ë£Œìœ¨']
            pred_data['ë‹´ë‹¹ì_ì´ìŠ¹ì¸ìˆ˜'] = manager_row['ìŠ¹ì¸ì™„ë£Œìˆ˜']
            pred_data['ë‹´ë‹¹ì_ì´ë¦¬ë“œìˆ˜'] = manager_row['ì´_ë¦¬ë“œìˆ˜']
            pred_data['ë‹´ë‹¹ì_í‰ê· ì ‘ì´‰'] = manager_row['í‰ê· _ì ‘ì´‰íšŸìˆ˜']
            pred_data['ë‹´ë‹¹ì_ê²½í—˜ì§€ìˆ˜'] = manager_row['ì´_ë¦¬ë“œìˆ˜'] / app_state.manager_performance['ì´_ë¦¬ë“œìˆ˜'].max()
            pred_data['ë‹´ë‹¹ì_íš¨ìœ¨ì„±'] = manager_row['ìŠ¹ì¸ì™„ë£Œìœ¨'] / (manager_row['í‰ê· _ì ‘ì´‰íšŸìˆ˜'] + 1)

            # íŒë§¤ì²˜ ê´€ë ¨ í”¼ì²˜ (ì „ì²´ í‰ê·  ì‚¬ìš©)
            channel_stats = app_state.processed_data.groupby('íŒë§¤ì²˜')['is_approved'].mean()
            pred_data['íŒë§¤ì²˜_í‰ê· ìŠ¹ì¸ìœ¨'] = channel_stats.get(channel, channel_stats.mean())

            # ìƒí˜¸ì‘ìš© í”¼ì²˜
            pred_data['ë‹´ë‹¹ìXíŒë§¤ì²˜_ì‹œë„ˆì§€'] = pred_data['ë‹´ë‹¹ì_ìŠ¹ì¸ìœ¨'] * pred_data['íŒë§¤ì²˜_í‰ê· ìŠ¹ì¸ìœ¨']

            # DataFrameìœ¼ë¡œ ë³€í™˜
            pred_df = pd.DataFrame([pred_data])

            # ì¸ì½”ë”©
            for col, encoder in app_state.encoders.items():
                if col in pred_df.columns:
                    try:
                        pred_df[f'{col}_encoded'] = encoder.transform(pred_df[col])
                    except:
                        pred_df[f'{col}_encoded'] = 0

            # ì˜ˆì¸¡
            try:
                X_pred = pred_df[app_state.feature_names].fillna(0)

                # ìŠ¤ì¼€ì¼ë§
                if app_state.scaler:
                    X_pred = app_state.scaler.transform(X_pred)

                # í”¼ì²˜ ì„ íƒ
                if app_state.feature_selector:
                    X_pred = app_state.feature_selector.transform(X_pred)

                # ì˜ˆì¸¡
                if app_state.ensemble_model:
                    # ì•™ìƒë¸” ëª¨ë¸ ìš°ì„  ì‚¬ìš©
                    pred_proba = app_state.ensemble_model.predict_proba(X_pred)[0, 1]
                else:
                    pred_proba = app_state.best_model.predict_proba(X_pred)[0, 1]

            except:
                pred_proba = 0.5

            # ì›Œí¬ë¡œë“œ ê³ ë ¤ (í˜„ì¬ ë¦¬ë“œìˆ˜ê°€ ë§ì€ ë‹´ë‹¹ìëŠ” í˜ë„í‹°)
            workload_penalty = 1 - (manager_row['ì´_ë¦¬ë“œìˆ˜'] / app_state.manager_performance['ì´_ë¦¬ë“œìˆ˜'].max()) * 0.1
            adjusted_proba = pred_proba * workload_penalty

            predictions.append({
                'ë‹´ë‹¹ì': manager,
                'ì˜ˆìƒ_ìŠ¹ì¸í™•ë¥ ': pred_proba,
                'ì¡°ì •_ìŠ¹ì¸í™•ë¥ ': adjusted_proba,
                'ê³¼ê±°_ìŠ¹ì¸ìœ¨': manager_row['ìŠ¹ì¸ì™„ë£Œìœ¨'],
                'í˜„ì¬_ë¦¬ë“œìˆ˜': manager_row['ì´_ë¦¬ë“œìˆ˜'],
                'í‰ê· _ì ‘ì´‰íšŸìˆ˜': manager_row['í‰ê· _ì ‘ì´‰íšŸìˆ˜'],
                'ë‹´ë‹¹ì_íš¨ìœ¨ì„±': pred_data['ë‹´ë‹¹ì_íš¨ìœ¨ì„±'],
                'ì¢…í•©ì ìˆ˜': adjusted_proba * 0.6 + manager_row['ìŠ¹ì¸ì™„ë£Œìœ¨'] * 0.3 + pred_data['ë‹´ë‹¹ì_íš¨ìœ¨ì„±'] * 0.1
            })

        if len(predictions) == 0:
            return "", "âŒ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ë‹´ë‹¹ìê°€ ì—†ìŠµë‹ˆë‹¤.", None

        # ê²°ê³¼ ì •ë ¬
        pred_df = pd.DataFrame(predictions).sort_values('ì¢…í•©ì ìˆ˜', ascending=False)

        # ìµœì  ë‹´ë‹¹ì ì„ íƒ
        best_manager = pred_df.iloc[0]['ë‹´ë‹¹ì']
        best_prob = pred_df.iloc[0]['ì˜ˆìƒ_ìŠ¹ì¸í™•ë¥ ']
        best_adjusted_prob = pred_df.iloc[0]['ì¡°ì •_ìŠ¹ì¸í™•ë¥ ']

        # ì‹œê°í™” (ê°œì„ ëœ ë²„ì „)
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                'ë‹´ë‹¹ìë³„ ì˜ˆìƒ ìŠ¹ì¸í™•ë¥ ',
                'ì¢…í•© ì ìˆ˜ (ì›Œí¬ë¡œë“œ ê³ ë ¤)',
                'ë‹´ë‹¹ì íš¨ìœ¨ì„± ì§€í‘œ',
                'ì¶”ì²œ ê·¼ê±° ë¶„ì„'
            ),
            specs=[[{"type": "bar"}, {"type": "bar"}],
                   [{"type": "scatter"}, {"type": "bar"}]]
        )

        # ìƒìœ„ 5ëª…ë§Œ í‘œì‹œ
        top_5 = pred_df.head(5)

        # 1. ì˜ˆìƒ ìŠ¹ì¸í™•ë¥ 
        fig.add_trace(
            go.Bar(
                x=top_5['ë‹´ë‹¹ì'],
                y=top_5['ì˜ˆìƒ_ìŠ¹ì¸í™•ë¥ '],
                name='ì˜ˆìƒ ìŠ¹ì¸í™•ë¥ ',
                marker_color='lightblue',
                text=top_5['ì˜ˆìƒ_ìŠ¹ì¸í™•ë¥ '].apply(lambda x: f'{x:.1%}'),
                textposition='outside'
            ),
            row=1, col=1
        )

        # ê³¼ê±° ìŠ¹ì¸ìœ¨ ì¶”ê°€
        fig.add_trace(
            go.Bar(
                x=top_5['ë‹´ë‹¹ì'],
                y=top_5['ê³¼ê±°_ìŠ¹ì¸ìœ¨'],
                name='ê³¼ê±° í‰ê· ',
                marker_color='lightgreen',
                text=top_5['ê³¼ê±°_ìŠ¹ì¸ìœ¨'].apply(lambda x: f'{x:.1%}'),
                textposition='outside'
            ),
            row=1, col=1
        )

        # 2. ì¢…í•© ì ìˆ˜
        fig.add_trace(
            go.Bar(
                x=top_5['ë‹´ë‹¹ì'],
                y=top_5['ì¢…í•©ì ìˆ˜'],
                name='ì¢…í•© ì ìˆ˜',
                marker_color='orange',
                text=top_5['ì¢…í•©ì ìˆ˜'].apply(lambda x: f'{x:.3f}'),
                textposition='outside',
                showlegend=False
            ),
            row=1, col=2
        )

        # 3. íš¨ìœ¨ì„± vs ì›Œí¬ë¡œë“œ
        fig.add_trace(
            go.Scatter(
                x=top_5['í˜„ì¬_ë¦¬ë“œìˆ˜'],
                y=top_5['ë‹´ë‹¹ì_íš¨ìœ¨ì„±'],
                mode='markers+text',
                marker=dict(
                    size=top_5['ì˜ˆìƒ_ìŠ¹ì¸í™•ë¥ '] * 50,
                    color=top_5['ì¢…í•©ì ìˆ˜'],
                    colorscale='RdYlGn',
                    showscale=True
                ),
                text=top_5['ë‹´ë‹¹ì'],
                textposition='top center',
                name='íš¨ìœ¨ì„±',
                showlegend=False
            ),
            row=2, col=1
        )

        # 4. ì¶”ì²œ ê·¼ê±° ìš”ì•½
        factors = ['ì˜ˆìƒ ìŠ¹ì¸ë¥ ', 'ê³¼ê±° ì„±ê³¼', 'íš¨ìœ¨ì„±', 'ì›Œí¬ë¡œë“œ']
        best_factors = [
            pred_df.iloc[0]['ì˜ˆìƒ_ìŠ¹ì¸í™•ë¥ '],
            pred_df.iloc[0]['ê³¼ê±°_ìŠ¹ì¸ìœ¨'],
            pred_df.iloc[0]['ë‹´ë‹¹ì_íš¨ìœ¨ì„±'],
            1 - pred_df.iloc[0]['í˜„ì¬_ë¦¬ë“œìˆ˜'] / pred_df['í˜„ì¬_ë¦¬ë“œìˆ˜'].max()
        ]

        fig.add_trace(
            go.Bar(
                x=factors,
                y=best_factors,
                name='í‰ê°€ ì§€í‘œ',
                marker_color=['blue', 'green', 'orange', 'red'],
                text=[f'{v:.2f}' for v in best_factors],
                textposition='outside',
                showlegend=False
            ),
            row=2, col=2
        )

        # ë ˆì´ì•„ì›ƒ ì„¤ì •
        fig.update_layout(
            height=800,
            showlegend=True,
            title_text=f'AI ê¸°ë°˜ ìµœì  ë‹´ë‹¹ì ì¶”ì²œ: {best_manager}'
        )

        fig.update_xaxes(title_text="ë‹´ë‹¹ì", row=1, col=1)
        fig.update_xaxes(title_text="ë‹´ë‹¹ì", row=1, col=2)
        fig.update_xaxes(title_text="í˜„ì¬ ë¦¬ë“œìˆ˜", row=2, col=1)
        fig.update_xaxes(title_text="í‰ê°€ ìš”ì†Œ", row=2, col=2)

        fig.update_yaxes(title_text="í™•ë¥ ", row=1, col=1)
        fig.update_yaxes(title_text="ì ìˆ˜", row=1, col=2)
        fig.update_yaxes(title_text="íš¨ìœ¨ì„± ì§€ìˆ˜", row=2, col=1)
        fig.update_yaxes(title_text="ì§€í‘œê°’", row=2, col=2)

        # ì¶”ì²œ ì´ìœ  ìƒì„± (ë” ìƒì„¸í•œ ë²„ì „)
        recommendation_text = f"""ğŸ¯ ìµœì  ë‹´ë‹¹ì ì¶”ì²œ: {best_manager}

ğŸ“Š ì¶”ì²œ ê·¼ê±°:
- ì˜ˆìƒ ìŠ¹ì¸ í™•ë¥ : {best_prob:.1%} (ì¡°ì • í›„: {best_adjusted_prob:.1%})
- ê³¼ê±° í‰ê·  ìŠ¹ì¸ìœ¨: {pred_df.iloc[0]['ê³¼ê±°_ìŠ¹ì¸ìœ¨']:.1%}
- í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë¦¬ë“œ: {int(pred_df.iloc[0]['í˜„ì¬_ë¦¬ë“œìˆ˜'])}ê±´
- í‰ê·  ì ‘ì´‰ íšŸìˆ˜: {pred_df.iloc[0]['í‰ê· _ì ‘ì´‰íšŸìˆ˜']:.1f}íšŒ
- ë‹´ë‹¹ì íš¨ìœ¨ì„±: {pred_df.iloc[0]['ë‹´ë‹¹ì_íš¨ìœ¨ì„±']:.3f}

ğŸ’¡ ì¶”ì²œ ì´ìœ :
1. ì´ ë‹´ë‹¹ìëŠ” ìœ ì‚¬í•œ í”„ë¡œíŒŒì¼ì˜ ë¦¬ë“œì—ì„œ ë†’ì€ ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.
2. í˜„ì¬ ì›Œí¬ë¡œë“œê°€ ì ì ˆí•œ ìˆ˜ì¤€ìœ¼ë¡œ ì¶”ê°€ ë¦¬ë“œ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
3. íš¨ìœ¨ì„± ì§€í‘œê°€ ìš°ìˆ˜í•˜ì—¬ ì ì€ ì ‘ì´‰ìœ¼ë¡œë„ ë†’ì€ ì „í™˜ìœ¨ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
4. AI ëª¨ë¸ì´ ì´ ë¦¬ë“œì˜ íŠ¹ì„±ê³¼ ë‹´ë‹¹ìì˜ ê°•ì ì´ ë§¤ì¹˜ëœë‹¤ê³  ì˜ˆì¸¡í•©ë‹ˆë‹¤.

ğŸ“ˆ ëŒ€ì•ˆ ë‹´ë‹¹ì:
"""

        for i in range(1, min(3, len(pred_df))):
            alt = pred_df.iloc[i]
            recommendation_text += f"\n{i+1}. {alt['ë‹´ë‹¹ì']} (ì˜ˆìƒ: {alt['ì˜ˆìƒ_ìŠ¹ì¸í™•ë¥ ']:.1%}, ì¢…í•©: {alt['ì¢…í•©ì ìˆ˜']:.3f})"

        # ë¦¬ë“œ íŠ¹ì„± ìš”ì•½
        recommendation_text += f"""

ğŸ“‹ ë¦¬ë“œ íŠ¹ì„± ìš”ì•½:
- íŒë§¤ì²˜: {channel}
- ê³ ê° í”„ë¡œíŒŒì¼: {gender} / {type_}
- ë™ì˜ ì ìˆ˜: {new_lead['ë™ì˜ì ìˆ˜']}/3
- ì˜ˆìƒ ë©”ëª¨ í’ˆì§ˆ: {new_lead['ë©”ëª¨_í’ˆì§ˆì ìˆ˜']:.2f}
- ì‹œê°„ëŒ€ íŠ¹ì„±: {'ì£¼ë§' if new_lead['ì£¼ë§ì—¬ë¶€'] else 'í‰ì¼'}, {current_datetime.hour}ì‹œ
"""

        return best_manager, recommendation_text, fig

    except Exception as e:
        return "", f"âŒ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", None

# ================================================================================
# 12. Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„± (ê¸°ì¡´ ìœ ì§€)
# ================================================================================

def create_gradio_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""

    # CSS ìŠ¤íƒ€ì¼
    custom_css = """
    .gradio-container {
        font-family: 'Noto Sans KR', -apple-system, BlinkMacSystemFont, sans-serif !important;
    }
    .gr-button-primary {
        background-color: #2563eb !important;
        color: white !important;
    }
    .gr-button-primary:hover {
        background-color: #1d4ed8 !important;
    }
    .error-text {
        color: #dc2626 !important;
        font-weight: bold;
    }
    .success-text {
        color: #16a34a !important;
        font-weight: bold;
    }
    """

    with gr.Blocks(title="ë‹´ë‹¹ìë³„ ë¦¬ë“œ ì „í™˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ", theme=gr.themes.Soft(), css=custom_css) as app:

        # í—¤ë”
        gr.Markdown("""
        # ğŸ¯ ë‹´ë‹¹ìë³„ ë¦¬ë“œ ì „í™˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ

        ### ğŸ“Š AI ê¸°ë°˜ ë¦¬ë“œ ë°°ì • ìµœì í™” ë° ì„±ê³¼ ì˜ˆì¸¡ í”Œë«í¼

        ---
        """)

        # ë©”ì¸ íƒ­
        with gr.Tabs():

            # 1. ë°ì´í„° ì—…ë¡œë“œ ë° ì „ì²˜ë¦¬
            with gr.Tab("ğŸ“ ë°ì´í„° ê´€ë¦¬"):
                with gr.Row():
                    with gr.Column(scale=1):
                        file_input = gr.File(
                            label="ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ (tst_atalk.xlsx)",
                            file_types=[".xlsx", ".xls", ".csv"],
                            type="filepath"
                        )

                        with gr.Row():
                            upload_btn = gr.Button("ğŸ“¤ ë°ì´í„° ë¡œë“œ", variant="primary")
                            sample_btn = gr.Button("ğŸ² ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©", variant="secondary")

                        preprocess_btn = gr.Button("ğŸ”§ ì „ì²˜ë¦¬ ì‹¤í–‰", variant="primary", size="lg")

                    with gr.Column(scale=2):
                        data_info = gr.Textbox(
                            label="ë°ì´í„° ë¶„ì„ ê²°ê³¼",
                            lines=20,
                            interactive=False
                        )

                # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
                upload_btn.click(
                    fn=load_data,
                    inputs=[file_input],
                    outputs=[gr.State(), data_info]
                )

                sample_btn.click(
                    fn=lambda: load_data(None),
                    outputs=[gr.State(), data_info]
                )

                preprocess_btn.click(
                    fn=preprocess_for_prediction,
                    outputs=[gr.State(), data_info]
                )

            # 2. ë‹´ë‹¹ì ì„±ê³¼ ë¶„ì„
            with gr.Tab("ğŸ‘¥ ë‹´ë‹¹ì ì„±ê³¼ ë¶„ì„"):
                analyze_btn = gr.Button("ğŸ“Š ì„±ê³¼ ë¶„ì„ ì‹¤í–‰", variant="primary", size="lg")

                status_text = gr.Textbox(label="ìƒíƒœ", interactive=False, visible=True)

                with gr.Row():
                    perf_plot1 = gr.Plot(label="ì„±ê³¼ ë§¤íŠ¸ë¦­ìŠ¤")
                    perf_plot2 = gr.Plot(label="ìŠ¹ì¸ìœ¨ ìˆœìœ„")

                with gr.Row():
                    perf_plot3 = gr.Plot(label="ì¼ë³„ ì¶”ì´")
                    perf_plot4 = gr.Plot(label="ì ‘ì´‰ íš¨ìœ¨ì„±")

                def analyze_performance():
                    plots, message = create_manager_performance_dashboard()
                    if len(plots) >= 4:
                        return message, plots[0], plots[1], plots[2], plots[3]
                    elif len(plots) == 3:
                        return message, plots[0], plots[1], plots[2], None
                    elif len(plots) == 2:
                        return message, plots[0], plots[1], None, None
                    elif len(plots) == 1:
                        return message, plots[0], None, None, None
                    else:
                        return message, None, None, None, None

                analyze_btn.click(
                    fn=analyze_performance,
                    outputs=[status_text, perf_plot1, perf_plot2, perf_plot3, perf_plot4]
                )

            # 3. ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ
            with gr.Tab("ğŸ¤– AI ëª¨ë¸ í•™ìŠµ"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“… í•™ìŠµ ê¸°ê°„ ì„¤ì •")

                        # ë‚ ì§œ ì„ íƒ ì»´í¬ë„ŒíŠ¸
                        with gr.Row():
                            train_start_date = gr.Textbox(
                                label="í•™ìŠµ ì‹œì‘ì¼",
                                value="2024-01-01",
                                info="YYYY-MM-DD í˜•ì‹"
                            )
                            train_end_date = gr.Textbox(
                                label="í•™ìŠµ ì¢…ë£Œì¼",
                                value=datetime.now().strftime("%Y-%m-%d"),
                                info="YYYY-MM-DD í˜•ì‹"
                            )

                        gr.Markdown("### ğŸ”® ì˜ˆì¸¡ ê¸°ê°„ ì„¤ì •")

                        with gr.Row():
                            predict_start_date = gr.Textbox(
                                label="ì˜ˆì¸¡ ì‹œì‘ì¼",
                                value=datetime.now().strftime("%Y-%m-%d"),
                                info="YYYY-MM-DD í˜•ì‹"
                            )
                            predict_days = gr.Slider(
                                label="ì˜ˆì¸¡ ì¼ìˆ˜",
                                minimum=1,
                                maximum=90,
                                value=30,
                                step=1,
                                info="ë©°ì¹  í›„ê¹Œì§€ ì˜ˆì¸¡í• ì§€ ì„¤ì •"
                            )

                        train_btn = gr.Button("ğŸš€ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì‹œì‘", variant="primary", size="lg")

                        model_info = gr.Textbox(
                            label="ëª¨ë¸ í•™ìŠµ ê²°ê³¼",
                            lines=15,
                            interactive=False
                        )

                    with gr.Column(scale=2):
                        with gr.Tabs():
                            with gr.Tab("ğŸ“ˆ ë‹´ë‹¹ìë³„ ì˜ˆìƒ ì„±ê³¼"):
                                prediction_table = gr.Dataframe(
                                    label="ë‹´ë‹¹ìë³„ ì˜ˆì¸¡ ê²°ê³¼",
                                    interactive=False
                                )

                            with gr.Tab("ğŸ“Š ì‹œê³„ì—´ ì˜ˆì¸¡"):
                                time_series_plot = gr.Plot(
                                    label="ê¸°ê°„ë³„ ì˜ˆì¸¡ ì¶”ì´"
                                )

                def train_and_predict_with_dates(start_date, end_date, pred_start, pred_days):
                    try:
                        # ë‚ ì§œ í˜•ì‹ ê²€ì¦
                        start = pd.to_datetime(start_date)
                        end = pd.to_datetime(end_date)
                        pred_start_dt = pd.to_datetime(pred_start)

                        if start > end:
                            return "âŒ ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.", pd.DataFrame(), None

                        # ì§€ì •ëœ ê¸°ê°„ì˜ ë°ì´í„°ë§Œ í•„í„°ë§
                        filtered_data = filter_data_by_date(start, end)

                        if filtered_data is None or len(filtered_data) == 0:
                            return "âŒ ì„ íƒí•œ ê¸°ê°„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", pd.DataFrame(), None

                        # ëª¨ë¸ í•™ìŠµ
                        results, text = train_prediction_model_with_data(filtered_data)

                        if app_state.is_model_trained:
                            # ì§€ì •ëœ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì„±ê³¼ ì˜ˆì¸¡
                            pred_df = predict_manager_performance(pred_start_dt, pred_days)

                            # ì‹œê³„ì—´ ì˜ˆì¸¡
                            time_series_fig = create_time_series_prediction(pred_start_dt, pred_days)

                            # í•™ìŠµ ì •ë³´ ì¶”ê°€
                            text += f"\n\nğŸ“… í•™ìŠµ ê¸°ê°„: {start.date()} ~ {end.date()}"
                            text += f"\nğŸ”® ì˜ˆì¸¡ ê¸°ê°„: {pred_start_dt.date()} ~ {(pred_start_dt + pd.Timedelta(days=pred_days)).date()}"
                            text += f"\nğŸ“Š ì˜ˆì¸¡ ì¼ìˆ˜: {pred_days}ì¼"

                            return text, pred_df, time_series_fig

                        return text, pd.DataFrame(), None

                    except Exception as e:
                        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", pd.DataFrame(), None

                train_btn.click(
                    fn=train_and_predict_with_dates,
                    inputs=[train_start_date, train_end_date, predict_start_date, predict_days],
                    outputs=[model_info, prediction_table, time_series_plot]
                )

            # 4. ë¦¬ë“œ ë°°ì • ìµœì í™”
            with gr.Tab("ğŸ¯ ë¦¬ë“œ ë°°ì • ìµœì í™”"):
                gr.Markdown("### ì‹ ê·œ ë¦¬ë“œ ì •ë³´ ì…ë ¥")

                with gr.Row():
                    with gr.Column():
                        channel_input = gr.Dropdown(
                            label="íŒë§¤ì²˜",
                            choices=['Gë§ˆì¼“', 'CJëª°', '11ë²ˆê°€', 'SSG', 'ì˜¥ì…˜', 'ë„¤ì´ë²„', 'ì¿ íŒ¡', 'ê¸°íƒ€'],
                            value="Gë§ˆì¼“"
                        )
                        gender_input = gr.Radio(
                            label="ì„±ë³„",
                            choices=["ë‚¨ì", "ì—¬ì"],
                            value="ì—¬ì"
                        )
                        type_input = gr.Radio(
                            label="ìœ í˜•",
                            choices=["ê°œì¸", "ë²•ì¸"],
                            value="ê°œì¸"
                        )
                        memo_length = gr.Slider(
                            label="ë©”ëª¨ ê¸¸ì´",
                            minimum=0,
                            maximum=500,
                            value=100,
                            step=10
                        )
                        marketing_consent = gr.Checkbox(label="ë§ˆì¼€íŒ… ë™ì˜", value=True)
                        email_consent = gr.Checkbox(label="ì´ë©”ì¼ ìˆ˜ì‹  ë™ì˜", value=True)
                        sms_consent = gr.Checkbox(label="ë¬¸ì ìˆ˜ì‹  ë™ì˜", value=True)

                        assign_btn = gr.Button("ğŸ¯ ìµœì  ë‹´ë‹¹ì ì°¾ê¸°", variant="primary", size="lg")

                    with gr.Column():
                        best_manager = gr.Textbox(
                            label="ì¶”ì²œ ë‹´ë‹¹ì",
                            interactive=False
                        )
                        recommendation = gr.Textbox(
                            label="ì¶”ì²œ ìƒì„¸ ì •ë³´",
                            lines=15,
                            interactive=False
                        )
                        comparison_plot = gr.Plot(label="ë‹´ë‹¹ìë³„ ì˜ˆìƒ ì„±ê³¼")

                assign_btn.click(
                    fn=optimize_lead_assignment,
                    inputs=[channel_input, gender_input, type_input, memo_length,
                           marketing_consent, email_consent, sms_consent],
                    outputs=[best_manager, recommendation, comparison_plot]
                )

        # í‘¸í„°
        gr.Markdown("""
        ---

        ### ğŸ’¡ ì‚¬ìš© ê°€ì´ë“œ

        1. **ë°ì´í„° ì—…ë¡œë“œ**: tst_atalk.xlsx íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        2. **ì „ì²˜ë¦¬ ì‹¤í–‰**: ë°ì´í„°ë¥¼ ë¶„ì„ì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        3. **ì„±ê³¼ ë¶„ì„**: ë‹´ë‹¹ìë³„ í˜„ì¬ ì„±ê³¼ë¥¼ ë‹¤ê°ë„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
        4. **ëª¨ë¸ í•™ìŠµ**: AI ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ë‹´ë‹¹ìë³„ ì˜ˆìƒ ì„±ê³¼ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
        5. **ë¦¬ë“œ ë°°ì •**: ì‹ ê·œ ë¦¬ë“œì— ëŒ€í•´ ìµœì ì˜ ë‹´ë‹¹ìë¥¼ ì¶”ì²œë°›ìŠµë‹ˆë‹¤.

        ---

        Â© 2024 Lead Conversion Prediction System. All rights reserved.
        """)

    return app

# ================================================================================
# 13. ë©”ì¸ ì‹¤í–‰
# ================================================================================

if __name__ == "__main__":
    print("ğŸš€ ë‹´ë‹¹ìë³„ ë¦¬ë“œ ì „í™˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì‹œì‘ ì¤‘...")
    print("ğŸ“Œ ìƒ˜í”Œ ë°ì´í„°ë¡œ ë¹ ë¥´ê²Œ ì‹œì‘í•˜ë ¤ë©´ [ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©] ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”!")

    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
    print("\nğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
    print(f"- ê¸°ë³¸ ML ëª¨ë¸: âœ… (Random Forest, XGBoost, LightGBM ë“±)")
    print(f"- LSTM (ì‹œê³„ì—´): {'âœ…' if TENSORFLOW_AVAILABLE else 'âŒ (TensorFlow í•„ìš”)'}")
    print(f"- ARIMA (ì‹œê³„ì—´): {'âœ…' if STATSMODELS_AVAILABLE else 'âŒ (statsmodels/pmdarima í•„ìš”)'}")
    print(f"- Prophet (ì‹œê³„ì—´): {'âœ…' if PROPHET_AVAILABLE else 'âŒ (prophet í•„ìš”)'}")
    print(f"- CatBoost: {'âœ…' if CATBOOST_AVAILABLE else 'âŒ (catboost í•„ìš”)'}")
    print(f"- í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: {'âœ…' if OPTUNA_AVAILABLE else 'âŒ (optuna í•„ìš”)'}")
    print(f"- í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬: {'âœ…' if IMBALANCED_AVAILABLE else 'âŒ (imbalanced-learn í•„ìš”)'}")

    print("\nğŸ’¡ ì‹œê³„ì—´ ëª¨ë¸ì´ ì—†ì–´ë„ ê¸°ë³¸ ML ëª¨ë¸ë§Œìœ¼ë¡œ ì¶©ë¶„í•œ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # Gradio ì•± ìƒì„± ë° ì‹¤í–‰
    app = create_gradio_interface()

    # ì‹¤í–‰
    app.launch(
        share=True,  # ê³µìœ  ë§í¬ ìƒì„±
        debug=False,
        quiet=False
    )

    print("âœ… ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")